{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2S8Pn6mG0gi1",
        "3Sq6kK1abafk",
        "0eb74bb2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio LSTM-GRU\n",
        "\n",
        "En el presente cuaderno construirás dos redes neuronales recurrentes, una LSTM (memoria larga a corto plazo) y una GRU (unidad recurrente cerrada). Aplicando ambos modelos al modelamiento y predicción de la inflación en Colombia. Las LSTM fueron introducidas por [Hochreiter & Schmidhuber](https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory) con el objetivo de mitigar el problema del desvanecimiento de gradientes en redes neuronales recurrentes, mientras que las GRU fueron introducidas por [Cho et al.](https://arxiv.org/pdf/1412.3555) como una versión simplificada de la memoria larga a corto plazo.\n",
        "\n",
        "**Al finalizar este laboratorio serás capaz de:**\n",
        "- Implementar estados ocultos de arquitecturas LSTM y GRU utilizando Numpy y Keras.\n",
        "- Implementar dos modelos completos recurrentes para la predicción de series de tiempo.\n",
        "- Entender el funcionamiento y el potencial de las redes neuronales recurrentes aplicado a problemas económicos, financieros y actuariales.\n",
        "\n",
        "Para este laboratorio utilizarás Keras y Numpy.\n",
        "\n",
        "Antes de ver el problema, corre la celda de abajo para cargar las librerias requeridas y los archivos auxiliares desde el [repositorio](https://github.com/MateoOrtiz001/Laboratory-LSTM-GRU-Inflation)."
      ],
      "metadata": {
        "id": "cu1GI8Ctdjq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "!git clone https://github.com/MateoOrtiz001/Laboratory-LSTM-GRU-Inflation\n",
        "sys.path.append('Laboratory-LSTM-GRU-Inflation')\n",
        "from utils import *\n",
        "from tester import *"
      ],
      "metadata": {
        "id": "z6y_9PnAsnT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabla de Contenido\n",
        "- [1 - Dataset](#1)\n",
        "- [2 - Problemas de gradientes en redes neuronales recurrentes](#2)\n",
        "- [3 - Construyendo una LSTM](#3)\n",
        "  - [3.1 - Construyendo el forward pass de una celda](#3-1)\n",
        "  - [3.2 - Construyendo el forward pass del modelo](#3-2)\n",
        "  - [3.3 - Construyendo la retropropagación de una celda](#3-3)\n",
        "  - [3.4 - Construyendo la repropropagación del modelo](#3-4)\n",
        "  - [3.5 - Entrenamiento del modelo](#3-5)\n",
        "  - [3.6 - Predicción del modelo](#3-6)\n",
        "- [4 - Construyendo una GRU](#4)\n",
        "  - [4.1 - Construyendo el forward pass de una celda](#4-1)\n",
        "  - [4.2 - Construyendo el forward pass del modelo](#4-2)\n",
        "  - [4.3 - Construyendo la retropropagación de una celda](#4-3)\n",
        "  - [4.4 - Construyendo la repropropagación del modelo](#4-4)\n",
        "  - [4.5 - Entrenamiento del modelo](#4-5)\n",
        "  - [4.6 - Predicción del modelo](#4-6)\n",
        "- [5 - Implementaciones con Keras](#5)\n",
        "- [6 - Predicciones con intervalos de confianza (Opcional)](#6)\n",
        "- [7 - Bibliografía](#7)"
      ],
      "metadata": {
        "id": "Gp25rzOM5Fwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"1\"></a>\n",
        "## Dataset\n",
        "El dataset seleccionado corresponde al histórico del indice mensual de la inflación en Colombia presentado por el banco de la república (disponible en su [base de datos](https://suameca.banrep.gov.co/buscador-de-series/#/)).\n",
        "\n",
        "Primero vamos a cargar el dataset directamente desde el repositorio."
      ],
      "metadata": {
        "id": "D3VDe-lEhy3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEJs80XKhqx1"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('Laboratory-LSTM-GRU-Inflation/dataset/Datos-Inflacion.xlsx', decimal=',')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['Inflacion'])\n",
        "plt.title('Inflacion a traves del tiempo')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Inflacion')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El conjunto de datos contiene el historial de inflación desde 1955. En primer lugar, recortaremos la información a partir del año 2000, ya que los patrones observados en los registros más antiguos difieren de los de las últimas dos décadas. Posteriormente, prepararemos las columnas para estandarizar los tipos de datos."
      ],
      "metadata": {
        "id": "oatMIRJ9tA1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertimos la columna 'Fecha' a objeto datetime y la seleccionamos como índice\n",
        "data['Fecha'] = pd.to_datetime(data['Fecha'], format='%d/%m/%Y', errors='coerce')\n",
        "data = data.dropna(subset=['Fecha']).set_index('Fecha')\n",
        "\n",
        "# Eliminamos filas sin datos en la columna 'Inflacion'\n",
        "data = data.dropna(subset=['Inflacion']).copy()\n",
        "\n",
        "# Dividimos los datos de la inflacion sobre 100 para tener el porcentaje\n",
        "data['Inflacion'] = pd.to_numeric(data['Inflacion'], errors='coerce') / 100\n",
        "\n",
        "# Filtramos los datos desde 2000\n",
        "data = data[data.index >= '2000-01-01']\n",
        "display(data.head())"
      ],
      "metadata": {
        "id": "bGsX91jesw_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['Inflacion'])\n",
        "plt.title('Inflacion a traves del tiempo')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Inflacion')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WEDu3umVvKlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a desarrollar modelos que tomen doce meses e intenten predecir el decimo tercer mes. Por ejemplo, para predecir la inflación a la fecha de diciembre de 2025 el modelo toma los meses de diciembre de 2024, enero, febreo, marzo, abril, mayo, junio, julio, agosto, septiembre, octubre y noviembre de 2025."
      ],
      "metadata": {
        "id": "ub_e0B02EG36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 1 - Secuencia de ventanas\n",
        "Para entrenar un modelo recurrente es necesario transformar la serie temporal en un conjunto de secuencias de entrada y salida.\n",
        "\n",
        "Este proceso se conoce como **sliding window** o ventana deslizante, y consiste en recorrer los datos históricos con una ventana de longitud fija que captura una secuencia de valores pasados (entrada), asociándola con uno o varios valores futuros (salida).\n",
        "\n",
        "Por ejemplo, si tenemos la siguiente serie temporal:\n",
        "\n",
        "$$ [x_1,x_2,\\dotsb, x_t  ]$$\n",
        "\n",
        "y tenemos una ventana ```n_steps_in = 3``` para retornar un solo valor ```n_steps_out = 1```. Entonces las muestras de entrenamiento son de la forma:\n",
        "\n",
        "* Entrada: $[x_1,x_2,x_3]$. Salida: $x_4$\n",
        "* Entrada: $[x_2,x_3,x_4]$. Salida: $x_5$.\n",
        "* Entrada: $[x_{i-1},x_i,x_{i+1}]$. Salida: $x_{i+2}$.\n",
        "\n",
        "**Instrucciones**:\n",
        "-  Fija el número de iteraciones para aplicar la serie deslizante.\n",
        "-  Agrega al arreglo de las secuencias de entrada `X` los datos desde el instante actual `i` hasta el número de entradas `i + n_steps_in`.\n",
        "-  Agrega al arreglo de valores objetivo `Y` los datos desde el número de entradas `i + n_steps_in` hasta el número de salidas `i + n_steps_in + n_steps_out`."
      ],
      "metadata": {
        "id": "vAm9b1cSwiOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: create_sequences\n",
        "\n",
        "def create_sequences(data, n_steps_in, n_steps_out):\n",
        "  \"\"\"\n",
        "  Crea secuencias de entrada-salida para RNN usando sliding window\n",
        "  Argumentos:\n",
        "  data -- array 1D con los datos de inflación\n",
        "  n_steps_in -- número de pasos temporales de entrada (ventana)\n",
        "  n_steps_out -- número de pasos a predecir (tamaño de la salida).\n",
        "\n",
        "  Retorna:\n",
        "  X -- array con secuencias de entrada, shape (n_samples, n_steps_in)\n",
        "  Y -- array con valores objetivo, shape (n_samples, n_steps_out)\n",
        "  \"\"\"\n",
        "  X, Y = [], []\n",
        "  ### START CODE HERE ###\n",
        "  # Recorremos la serie aplicando la ventana deslizante\n",
        "  for i in range( ??? ):\n",
        "    # Secuencia de entrada: ventana de n_steps_in valores. X.append(...)\n",
        "    X.append( ??? )\n",
        "    # Valor objetivo: el siguiente valor después de la ventana. Y.append(...)\n",
        "    Y.append( ??? )\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return np.array(X), np.array(Y)"
      ],
      "metadata": {
        "id": "O-YAzSguwbpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebab15b6"
      },
      "source": [
        "create_sequences_test(create_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datos esperados:**\n",
        "\n",
        "Primer secuencia de entrada (X):\n",
        "```\n",
        "[0.37454012 0.95071431 0.73199394 0.59865848 0.15601864]\n",
        "```\n",
        "\n",
        "Primer secuencia de salida (Y):\n",
        "```\n",
        "[0.15599452 0.05808361]\n",
        "```"
      ],
      "metadata": {
        "id": "NHGRwjHG1MNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 2 - Forma de los datos\n",
        "La entrada ```x``` de los estados ocultos de nuestro modelo tienen la forma ```(n_x, m, T_x)```, que corresponde al número de neuronas, al número de muestras y a la longitud de la secuencia. Mientras que la salida ```y``` debe tener la forma ```(n_y,m,1)```, que corresponde al número de salidas, al tamaño de la muestra y a la única predicción que vamos a realizar.\n",
        "\n",
        "**Instrucciones**:\n",
        "-  Cambia la forma de los datos de entrada `X`y `Y` en las variables `X_reshaped` y `Y_reshaped`, respectivamente.\n",
        "\n",
        "#### Pista:\n",
        "-  Puede ser útil revisar la función ```reshape()``` en [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html).\n"
      ],
      "metadata": {
        "id": "T-4bOW63jN4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: reshape_data\n",
        "\n",
        "def reshape_data(X, Y):\n",
        "  \"\"\"\n",
        "  Transforma datos al formato requerido para la LSTM y la red GRU.\n",
        "  Los modelos esperan:\n",
        "  - X: shape (n_x, m, T_x) donde:\n",
        "    n_x = número de features\n",
        "    m = batch size (número de muestras)\n",
        "    T_x = longitud de secuencia temporal\n",
        "  - Y: shape (n_y, m, 1) donde:\n",
        "    n_y = número de salidas\n",
        "    m = batch size\n",
        "\n",
        "  Argumentos:\n",
        "  X -- array shape (m, T_x)\n",
        "  Y -- array shape (m, n_steps_out)\n",
        "\n",
        "  Retorna:\n",
        "  X_reshaped -- array shape (n_x, m, T_x)\n",
        "  Y_reshaped -- array shape (n_y, m, 1)\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  X_reshaped =   # X: de (m, T_x) a (1, m, T_x)\n",
        "  Y_reshaped =   # Y: de (m, n_steps_out) a (n_steps_out, m, 1)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return X_reshaped, Y_reshaped"
      ],
      "metadata": {
        "id": "kNT3tDYd1S2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "149f8684"
      },
      "source": [
        "reshape_data_test(reshape_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparación del conjunto de entrenamiento/prueba\n",
        "Primero normalizamos los datos. Posteriormente creamos las secuencias con la función definida anteriormente."
      ],
      "metadata": {
        "id": "hdVrv2eAFGUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizamos normalización con escala estándar\n",
        "scaler = StandardScaler()\n",
        "inflation_scaled = scaler.fit_transform(data[['Inflacion']].values)\n",
        "# Configuración de ventana temporal\n",
        "n_steps_in = 12  # Usar 12 meses previos para predecir el siguiente\n",
        "n_steps_out = 1  # Predecir 1 mes adelante\n",
        "\n",
        "X_seq, y_seq = create_sequences(inflation_scaled, n_steps_in, n_steps_out)"
      ],
      "metadata": {
        "id": "LTRH6c7I9Nek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora dividimos el conjunto de secuencias en 80% para entrenamiento y el 20% para prueba.\n",
        "Además, para entrenar los modelos con Keras, necesitamos mantener la misma forma."
      ],
      "metadata": {
        "id": "k3Ps027_FlrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento con el dataset\n",
        "train_size = int(len(X_seq) * 0.8)  # 80% entrenamiento, 20% test\n",
        "\n",
        "X_train_keras = X_seq[:train_size]\n",
        "y_train_keras = y_seq[:train_size]\n",
        "X_test_keras = X_seq[train_size:]\n",
        "y_test_keras = y_seq[train_size:]"
      ],
      "metadata": {
        "id": "iUDxzy08846P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente cambiamos la forma de los datos para el entrenamiento para los modelos hechos en NumPy."
      ],
      "metadata": {
        "id": "0cKOF6QaFvjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = reshape_data(X_train_keras, y_train_keras)\n",
        "X_test, y_test = reshape_data(X_test_keras, y_test_keras)"
      ],
      "metadata": {
        "id": "GaQHVYrp88fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"a\"></a>\n",
        "## Problemas de gradientes en redes neuronales recurrentes\n",
        "\n",
        "Sabemos que una red neuronal recurrente se define como una composición de funciones no lineales, usualmente $\\tanh$ y $\\sigma$ como se observa en las siguientes ecuaciones.\n",
        "\n",
        "$$ \\hat{y} = \\tanh \\left(W_y h^{\\langle t\\rangle} + b_y  \\right) $$\n",
        "$$ h^{\\langle t\\rangle} = \\sigma \\left( W_h \\left[h^{\\langle t-1\\rangle}; x^{\\langle t\\rangle}  \\right] + b_h  \\right) $$\n",
        "\n",
        "Si en algún paso $h^{\\langle t\\rangle}$ los cómputos de la retropropagación se hacen muy pequeños entonces se corre el riesgo de propagar este error a través de la composición, esta propagación de error hace que en algún momento el valor de los parámetros convergan al $0$ numérico, matando las neuronas.\n",
        "\n",
        "Para contrarrestar estos problemas numéricos, las arquitecturas LSTM y GRU operan los valores de los estados ocultos con *compuertas* que determinan los valores a propagar a través de la red."
      ],
      "metadata": {
        "id": "DR9QDyXYFrzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3\"></a>\n",
        "## Primer Modelo: Construyendo una LSTM"
      ],
      "metadata": {
        "id": "idq5HVFsTDNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La arquitectura de la celda LSTM puede verse en la siguiente Figura.\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/MateoOrtiz001/Laboratory-LSTM-GRU-Inflation/main/imagenes/LSTM01.png\" width=\"500\">\n",
        "</center>\n",
        "Se tienen tres compuertas $\\Gamma_f$,$\\Gamma_u$ y $\\Gamma_o$, una célula de memoria para el estado $c^{\\langle t\\rangle}$ y un candidato a dicha célula $\\tilde{c}^{\\langle t\\rangle}$.\n",
        "\n",
        "-  *Compuerta de olvido* $\\Gamma_f$: es un tensor que contiene valores entre $0$ y $1$.\n",
        "    - Si una unidad en esta compuerta está muy cerca del 0 entonces la LSTM \"olvidará\" el estado guardado en la correspondiente unidad de la célula de memoria previa.\n",
        "    - Si una unidad en esta compuerta está muy cerca del 1 entonces la LSTM recordará fuertemente su correspondiente valor guardado en la unidad de la célula de memoria.\n",
        "\n",
        "    $$\\Gamma_f = \\sigma \\left(W_f \\left[h^{\\langle t-1\\rangle}; x^{\\langle t\\rangle}\\right] + b_f\\right)  \\tag{1}$$\n",
        "\n",
        "-  *Célula de memoria candidata* $\\tilde{c}^{\\langle t\\rangle}$: es un tensor que contiene información del estado de tiempo actual que podría ser guardado en la célula de memoria actual $c^{\\langle t\\rangle}$.\n",
        "    - Como los datos están normalizados, este tensor contiene valores entre $-1$ y $1$.\n",
        "\n",
        "    $$\\tilde{c}^{\\langle t\\rangle} = \\tanh\\left(W_c \\left[h^{\\langle t-1\\rangle}; x^{\\langle t\\rangle}\\right] + b_c\\right) \\tag{2} $$\n",
        "\n",
        "-  *Compuerta de actualización* $\\Gamma_u$: es un tensor que contiene valores entre $0$ y $1$.\n",
        "    - Esta compuerta decide qué valores del candidato $\\tilde{c}^{\\langle t\\rangle}$ son pasados a la célula de memoria $c^{\\langle t\\rangle}$. Las unidades cercanas al 0 no dejarán pasar valores, mientras que las más cercanas al 1 dejarán pasar fuertemente los valores.\n",
        "\n",
        "    $$\\Gamma_u = \\sigma \\left(W_u \\left[h^{\\langle t-1\\rangle}; x^{\\langle t\\rangle}\\right] + b_u\\right)  \\tag{3}$$\n",
        "\n",
        "-  *Célula de memoria* $c^{\\langle t\\rangle}$: como su nombre indica, es la \"memoria\" que será pasada a estados ocultos posteriores.\n",
        "    - Es simplemente la combinación entre la célula de memoria del estado anterior y el valor candidato.\n",
        "\n",
        "    $$c^{\\langle t\\rangle} = \\Gamma_f^{\\langle t\\rangle} * c^{\\langle t-1\\rangle} + \\Gamma_u^{\\langle t\\rangle} * \\tilde{c}^{\\langle t\\rangle} \\tag{4}$$\n",
        "\n",
        "-  *Compuerta de salida* $\\Gamma_o$: es un tensor que contiene valores entre $0$ y $1$.\n",
        "    - Esta compuerta decide qué enviar a la salida del estado oculto.\n",
        "\n",
        "    $$\\Gamma_o = \\sigma \\left(W_o \\left[h^{\\langle t-1\\rangle}; x^{\\langle t\\rangle}\\right] + b_o\\right)  \\tag{5}$$\n",
        "\n",
        "-  *Estado oculto* $h^{\\langle t\\rangle}$: es un tensor que sirve como entrada para el siguiente paso de tiempo.\n",
        "    - Es usado para determinar las compuertas del siguiente paso del tiempo.\n",
        "    - Es simplemente la combinación entre la compuerta de salida $\\Gamma_o$ y el estado de memoria $c\\langle t\\rangle$, este último sobre la función $\\tanh$.\n",
        "    - La compuerta actúa para determinar qué valores de la célula de memoria deben ser incluídos en el estado oculto actual.\n",
        "\n",
        "    $$h^{\\langle t\\rangle} = \\Gamma_o * \\tanh\\left(c^{\\langle t\\rangle}\\right) \\tag{6}$$"
      ],
      "metadata": {
        "id": "GR-kMlnfuNO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3-1\"></a>\n",
        "### Ejercicio 3 - Celda del Foward Pass\n",
        "Implementa la celda LSTM observada en la Figura.\n",
        "\n",
        "**Intrucciones**:\n",
        "1. Concatena el estado oculto $h^{\\langle t-1 \\rangle}$ $x^{\\langle t \\rangle}$ en una sola matriz:  \n",
        "\n",
        "$$concat = \\begin{bmatrix} h^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$$  \n",
        "\n",
        "2. Computa todas las fórmulas (1 hasta 6) para las compuertas, el estado oculto y el estado de la célula.\n",
        "3. Para este problema, no tienes que computar la predicción $y^{\\langle t \\rangle}$ en todas las celdas, luego no deberás implementarla aquí.\n",
        "\n",
        "#### Pistas\n",
        "* Puedes usar [numpy.concatenate](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html).  Revisa qué valor usar para el parámetro `axis`.\n",
        "* Las funciones `sigmoid()` y `softmax` se importan desde `utils.py`.\n",
        "* La documentación sobre [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html)\n",
        "* Usa [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) para la multiplicación de matrices."
      ],
      "metadata": {
        "id": "7oqJCxbtcOY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función adaptada del curso de deepLearningAi. Lab15a: Building a Recurrent Neuronal Network Step by Step\n",
        "# Función calificadora: lstm_cell_forward\n",
        "\n",
        "def lstm_cell_forward(xt, h_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implementar un solo paso de la celda LSTM como se describe en la Figura (4)\n",
        "    Argumentos:\n",
        "    xt -- la entrada en el timestep \"t\", numpy array de forma (n_x, m)\n",
        "    h_prev -- Estado oculto en el tiempo \"t-1\", numpy array de la forma (n_a,m)\n",
        "    c_prev -- Celula de memoria en el tiempo \"t-1\", numpy array of shape (n_a, m)\n",
        "    parametros -- diccionario de python que contiene:\n",
        "                        Wf -- Matriz de pesos de la compuerta de olvido, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bf -- Sesgo de la compuerta de olvido, numpy array de forma (n_a, 1)\n",
        "                        Wu -- Matriz de pesos de la compuerta de actualización, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bu -- Sesgo de la compuerta de actualización, numpy array de forma (n_a, 1)\n",
        "                        Wc -- Matriz de pesos de la primera \"tanh\", numpy array de forma (n_h, n_h + n_x)\n",
        "                        bc --  Sesgo de la primera \"tanh\", numpy array de forma (n_h, 1)\n",
        "                        Wo -- Matriz de pesos de la compuerta de salida, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bo --  Sesgo de la compuerta de salida, numpy array de forma (n_h, 1)\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    h_next -- siguiente estado oculto, de forma (n_a, m)\n",
        "    c_next -- siguiente estado de memoria, de forma (n_a, m)\n",
        "    cache -- tupla de valores necesarios para el paso hacia atrás, contiene (h_next, c_next, h_prev, c_prev, xt, parameters)\n",
        "\n",
        "    Nota: ft/ut/ot significan las compuertas de olvido/actualización/salida, cct significa el valor candidato (c tilde),\n",
        "          c significa el estado de la celda (memoria)\n",
        "    \"\"\"\n",
        "\n",
        "    # Recuperar parámetros de \"parameters\"\n",
        "    Wf = parameters[\"Wf\"] # peso de la compuerta de olvido\n",
        "    bf = parameters[\"bf\"]\n",
        "    Wu = parameters[\"Wu\"] # peso de la compuerta de actualización (observar el nombre de la variable)\n",
        "    bu = parameters[\"bu\"] # (observar el nombre de la variable)\n",
        "    Wc = parameters[\"Wc\"] # peso del valor candidato\n",
        "    bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"] # peso de la compuerta de salida\n",
        "    bo = parameters[\"bo\"]\n",
        "\n",
        "\n",
        "    # Recuperar dimensiones de las formas de xt\n",
        "    n_x, m = xt.shape\n",
        "    n_a, _ = h_prev.shape # Asumiendo que la forma de h_prev es (n_a, m)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Concatenar h_prev y xt\n",
        "    concat =\n",
        "\n",
        "    # Calcular valores para ft, ut, cct, c_next, ot, h_next usando las fórmulas dadas en la figura (4)\n",
        "    ft =\n",
        "    ut =\n",
        "    cct =\n",
        "    c_next =\n",
        "    ot =\n",
        "    h_next =\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # almacenar valores necesarios para la propagación hacia atrás en la caché\n",
        "    cache = (h_next, c_next, h_prev, c_prev, ft, ut, cct, ot, xt, parameters)\n",
        "\n",
        "    return h_next, c_next, cache"
      ],
      "metadata": {
        "id": "hqjE9sxNvAFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_cell_forward_test(lstm_cell_forward)"
      ],
      "metadata": {
        "id": "wWl5MDFcSOMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3-2\"></a>\n",
        "### Ejercicio 4 - Forward Pass"
      ],
      "metadata": {
        "id": "Caaf5LsMcdvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya haz implementado un paso de una LSTM, ahora puedes iterar sobre toda la secuencia utilizando un bucle sobre todas las entradas $T_x$.\n",
        "\n",
        "Implementa el modelo LSTM iterando las celdas hasta los $T_x$ pasos de tiempo.\n",
        "\n",
        "**Intrucciones**:\n",
        "-  Obtén las dimensiones $n_x,n_h,n_y,m,T_x$ de las formas de las variables `x`y `parameters`.\n",
        "-  Inicializa los tensores 3D $h,c$ y $y$\n",
        "    -  $h$: estado oculto, forma $(n_h,m,T_x)$.\n",
        "    -  $c$: célula de memoria, forma $(n_h,m,T_x)$.\n",
        "    -  $y$: predicción, forma $(n_h,m,T_y)$ donde $T_y=1$.\n",
        "-  Inicializa el tensor 2D $h^{\\langle t\\rangle}$.\n",
        "    -  $h^{\\langle t\\rangle}$ guarda el estado oculto para el paso de tiempo $t$. El nombre de la variable es `h_next`.\n",
        "    -  $h^{\\langle 0\\rangle}$, el estado oculto en el tiempo $0$, se pasa cuando se llama la función. El nombre de la variable es `h0`.\n",
        "    -  Inicializa $h^{\\langle t\\rangle}$ con el estado oculto $h^{\\langle 0\\rangle}$ que se pasa a la función.\n",
        "-  Inicializa $c^{\\langle t\\rangle}$ con ceros.\n",
        "    -  El nombre de la variable es `c_next`.\n",
        "-  Para cada instante de tiempo, realiza lo siguiente\n",
        "    -  Desde el tensor 3D $x$, toma la parte 2D $x^{\\langle t\\rangle}$ en el instante de tiempo $t$.\n",
        "    -  Llama la función `lstm_cell_forward` definida anteriormente, para obtener el estado oculto, la célula de memoria y el cache.\n",
        "    -  Guarda el estado oculto y la célula de memoria dentro de los tensores 3D.\n",
        "    -  Agrega el cache a los caches.\n",
        "\n",
        "*Nota*: En este caso observa que la predicción se retorna directamente como un tensor 2D."
      ],
      "metadata": {
        "id": "2zOAjsIRaTn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función adaptada del curso de deepLearningAi. Lab15a: Building a Recurrent Neuronal Network Step by Step\n",
        "# Función calificadora: lstm_forward\n",
        "\n",
        "def lstm_forward(x, h0, parameters):\n",
        "    \"\"\"\n",
        "    Implementa la propagación hacia adelante de la red neuronal recurrente usando una celda LSTM descrita en la Figura (4).\n",
        "\n",
        "    Argumentos:\n",
        "    x -- Datos de entrada para cada paso de tiempo, de forma (n_x, m, T_x).\n",
        "    h0 -- Estado oculto inicial, de forma (n_h, m)\n",
        "    parameters -- diccionario de python que contiene:\n",
        "                        Wf -- Matriz de pesos de la compuerta de olvido, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bf -- Sesgo de la compuerta de olvido, numpy array de forma (n_h, 1)\n",
        "                        Wu -- Matriz de pesos de la compuerta de actualización, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bu -- Sesgo de la compuerta de actualización, numpy array de forma (n_h, 1)\n",
        "                        Wc -- Matriz de pesos de la primera \"tanh\", numpy array de forma (n_h, n_h + n_x)\n",
        "                        bc -- Sesgo de la primera \"tanh\", numpy array de forma (n_h, 1)\n",
        "                        Wo -- Matriz de pesos de la compuerta de salida, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bo -- Sesgo de la compuerta de salida, numpy array de forma (n_h, 1)\n",
        "                        Wy -- Matriz de pesos que relaciona el estado oculto con la salida, numpy array de forma (n_y, n_h)\n",
        "                        by -- Sesgo que relaciona el estado oculto con la salida, numpy array de forma (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    h -- Estados ocultos para cada paso de tiempo, numpy array de forma (n_h, m, T_x)\n",
        "    y -- Predicciones para cada paso de tiempo, numpy array de forma (n_y, m)\n",
        "    c -- El valor del estado de la celda, numpy array de forma (n_h, m, T_x)\n",
        "    caches -- tupla de valores necesarios para el paso hacia atrás, contiene (lista de todas las cachés, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializar \"caches\", que rastreará la lista de todas las cachés\n",
        "    caches = []\n",
        "    Wy = parameters['Wy']\n",
        "    by = parameters['by']\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Recuperar dimensiones de las formas de x y parameters['Wy']\n",
        "    n_x, m, T_x =\n",
        "    n_y, n_h =\n",
        "\n",
        "    # inicializar \"h\", \"c\" y \"y\" con ceros\n",
        "    h =\n",
        "    c =\n",
        "    y =\n",
        "\n",
        "    # Inicializar h_next y c_next\n",
        "    h_next =\n",
        "    c_next =\n",
        "\n",
        "    # iterar sobre todos los pasos de tiempo\n",
        "    for t in range(T_x): # Iterar a través de todos los pasos de tiempo\n",
        "        # Obtener la sección 2D 'xt' de la entrada 3D 'x' en el paso de tiempo 't'\n",
        "        xt = x[:,:,t]\n",
        "        # Actualizar el siguiente estado oculto, el siguiente estado de memoria, calcular la predicción, obtener la caché\n",
        "        h_next, c_next, cache =\n",
        "\n",
        "        # Guardar el valor del nuevo estado oculto \"siguiente\" en h\n",
        "        h[:,:,t] =\n",
        "        # Guardar el valor del siguiente estado de celda\n",
        "        c[:,:,t]  =\n",
        "        # Añadir la caché a caches\n",
        "        caches.append( ??? )\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    y = np.dot(Wy,h[:,:,-1]) + by\n",
        "\n",
        "    # almacenar valores necesarios para la propagación hacia atrás en la caché\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return h, y, c, caches"
      ],
      "metadata": {
        "id": "KMx5fs6lcfnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_forward_test(lstm_forward)"
      ],
      "metadata": {
        "id": "bmfKx5IuTMlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3-3\"></a>\n",
        "### Ejercicio 5 - Celda del Backward Pass"
      ],
      "metadata": {
        "id": "Qh_H8iHmcoOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya con la propagación hacia adelante programada, ahora podemos calcular le retropropagación hacia atrás para el entrenamiento del modelo.\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/MateoOrtiz001/Laboratory-LSTM-GRU-Inflation/main/imagenes/LSTM02.png\" width=\"500\">\n",
        "</center>\n",
        "En cada celda calculamos la derivada de las variables asociadas a los pesos y sesgos de las compuertas, y las derivadas de la entrada $x^{\\langle t\\rangle}$ y de los estados anteriores $h^{\\langle t-1\\rangle}$ y $c^{\\langle t-1\\rangle}$.\n",
        "\n",
        "Como la retropropagación se hace hacia atrás de forma iterativa desde el instante $T_x$, en cada paso tenemos $dh^{\\langle t\\rangle}$ y $dc^{\\langle t\\rangle}$, llamados en el código `dh_next` y `dc_next` respectivamente. Con esto, las derivadas de cada compuerta está dada como sigue.\n",
        "\n",
        "\\begin{align}\n",
        "do^{\\langle t \\rangle} &= dh^{\\langle t \\rangle}*\\tanh(c^{\\langle t\\rangle}) * \\Gamma_o^{\\langle t \\rangle}*\\left(1-\\Gamma_o^{\\langle t \\rangle}\\right)\\tag{1} \\\\[8pt]\n",
        "d\\widetilde{c}^{\\langle t \\rangle} &= \\left(dc^{\\langle t\\rangle}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle}* (1-\\tanh^2(c^{\\langle t\\rangle})) * \\Gamma_u^{\\langle t \\rangle} * dh^{\\langle t \\rangle} \\right) * \\left(1-\\left(\\widetilde c^{\\langle t \\rangle}\\right)^2\\right) \\tag{2} \\\\[8pt]\n",
        "du^{\\langle t \\rangle} &= \\left(dc^{\\langle t\\rangle}*\\widetilde{c}^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle}* (1-\\tanh^2(c^{\\langle t\\rangle})) * \\widetilde{c}^{\\langle t \\rangle} * dh^{\\langle t \\rangle}\\right)*\\Gamma_u^{\\langle t \\rangle}*\\left(1-\\Gamma_u^{\\langle t \\rangle}\\right)\\tag{3} \\\\[8pt]\n",
        "df^{\\langle t \\rangle} &= \\left(dc^{\\langle t\\rangle}* c^{\\langle t-1\\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1-\\tanh^2(c^{\\langle t\\rangle}) * c^{\\langle t-1\\rangle} * dh^{\\langle t \\rangle}\\right)*\\Gamma_f^{\\langle t \\rangle}*\\left(1-\\Gamma_f^{\\langle t \\rangle}\\right)\\tag{4}\n",
        "\\end{align}\n",
        "\n",
        "Con esto podemos calcular las derivadas de los parámetros. Primero, la derivada de los pesos está dada como\n",
        "\n",
        "$ dW_f = df^{\\langle t \\rangle} \\begin{bmatrix} h^{\\langle t-1\\rangle} \\\\ x^{\\langle t\\rangle}\\end{bmatrix}^T \\tag{5} $\n",
        "$ dW_u = du^{\\langle t \\rangle} \\begin{bmatrix} h^{\\langle t-1\\rangle} \\\\ x^{\\langle t\\rangle}\\end{bmatrix}^T \\tag{6} $\n",
        "$ dW_c = d\\widetilde c^{\\langle t \\rangle} \\begin{bmatrix} h^{\\langle t-1\\rangle} \\\\ x^{\\langle t\\rangle}\\end{bmatrix}^T \\tag{7} $\n",
        "$ dW_o = do^{\\langle t \\rangle} \\begin{bmatrix} h^{\\langle t-1\\rangle} \\\\ x^{\\langle t\\rangle}\\end{bmatrix}^T \\tag{8}$\n",
        "\n",
        "Para calcular los sesgos $db_f, db_u, db_c, db_o$ solo necesitas sumar a través de todas las 'm' muestras (axis= 1) de $df^{\\langle t \\rangle}, du_u^{\\langle t \\rangle}, d\\widetilde c^{\\langle t \\rangle}, d_o^{\\langle t \\rangle}$ respectivamente. Observa que tienes que tener `keepdims = True`.\n",
        "\n",
        "$\\displaystyle db_f = \\sum_{batch}df^{\\langle t \\rangle}\\tag{9}$\n",
        "$\\displaystyle db_u = \\sum_{batch}du^{\\langle t \\rangle}\\tag{10}$\n",
        "$\\displaystyle db_c = \\sum_{batch}dc^{\\langle t \\rangle}\\tag{11}$\n",
        "$\\displaystyle db_o = \\sum_{batch}do^{\\langle t \\rangle}\\tag{12}$\n",
        "\n",
        "Finalmente, debes calcular la derivada con respeceto al estado anterior, la célula de memoria anterior y la salida.\n",
        "$ dh^{\\langle t-1\\rangle} = W_f^T df^{\\langle t \\rangle} + W_u^T   du^{\\langle t \\rangle}+ W_c^T d\\widetilde c^{\\langle t \\rangle} + W_o^T do^{\\langle t \\rangle} \\tag{13}$\n",
        "\n",
        "Aquí, para referirnos a la concatenación de los pesos para la ecuación (13) son los primeros $n_h$ valores, es decir, $W_f = W_f[:,:n_h]$, etc.\n",
        "\n",
        "$ dc^{\\langle t-1\\rangle} = dc^{\\langle t\\rangle}*\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh^2(c^{\\langle t\\rangle}))*\\Gamma_f^{\\langle t \\rangle}*dh^{\\langle t\\rangle} \\tag{14}$\n",
        "\n",
        "$ dx^{\\langle t \\rangle} = W_f^T df^{\\langle t \\rangle} + W_u^T  du^{\\langle t \\rangle}+ W_c^T d\\widetilde c^{\\langle t \\rangle} + W_o^T do^{\\langle t \\rangle}\\tag{15} $\n",
        "\n",
        "en donde los pesos para la ecuación (15) van desde $n_a$ hasta el final, es decir $W_f = W_f[:,n_a:]$, etc.\n",
        "\n",
        "**Instrucciones:**\n",
        "-  Implementa las ecuaciones (1) - (15) de arriba.\n",
        "    - $do^{\\langle t \\rangle}$ es representado por  `dot`,    \n",
        "    - $d\\widetilde{c}^{\\langle t \\rangle}$ es representado por  `dcct`,  \n",
        "    - $du^{\\langle t \\rangle}$ es representado por  `dot`,  \n",
        "    - $df^{\\langle t \\rangle}$ es representado por `dft`."
      ],
      "metadata": {
        "id": "XomKxgPPp6ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función adaptada del curso de deepLearningAi. Lab15a: Building a Recurrent Neuronal Network Step by Step\n",
        "# Función calificadora: lstm_cell_backward\n",
        "\n",
        "def lstm_cell_backward(dh_next, dc_next, cache):\n",
        "    \"\"\"\n",
        "    Implementa el paso hacia atrás para la celda LSTM (un solo paso de tiempo).\n",
        "\n",
        "    Argumentos:\n",
        "    dh_next -- Gradientes del siguiente estado oculto, de forma (n_a, m)\n",
        "    dc_next -- Gradientes del siguiente estado de celda, de forma (n_a, m)\n",
        "    cache -- caché que almacena información del paso hacia adelante\n",
        "\n",
        "    Returns:\n",
        "    gradients -- diccionario de python que contiene:\n",
        "                        dxt -- Gradiente de los datos de entrada en el paso de tiempo t, de forma (n_x, m)\n",
        "                        dh_prev -- Gradiente con respecto al estado oculto anterior, numpy array de forma (n_a, m)\n",
        "                        dc_prev -- Gradiente con respecto al estado de memoria anterior, de forma (n_a, m, T_x)\n",
        "                        dWf -- Gradiente con respecto a la matriz de pesos de la compuerta de olvido, numpy array de forma (n_a, n_a + n_x)\n",
        "                        dWu -- Gradiente con respecto a la matriz de pesos de la compuerta de actualización, numpy array de forma (n_a, n_a + n_x)\n",
        "                        dWc -- Gradiente con respecto a la matriz de pesos de la compuerta de memoria, numpy array de forma (n_a, n_a + n_x)\n",
        "                        dWo -- Gradiente con respecto a la matriz de pesos de la compuerta de salida, numpy array de forma (n_a, n_a + n_x)\n",
        "                        dbf -- Gradiente con respecto a los sesgos de la compuerta de olvido, de forma (n_a, 1)\n",
        "                        dbu -- Gradiente con respecto a los sesgos de la compuerta de actualización, de forma (n_a, 1)\n",
        "                        dbc -- Gradiente con respecto a los sesgos de la compuerta de memoria, de forma (n_a, 1)\n",
        "                        dbo -- Gradiente con respecto a los sesgos de la compuerta de salida, de forma (n_a, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Recuperar información de \"cache\"\n",
        "    (h_next, c_next, h_prev, c_prev, ft, ut, cct, ot, xt, parameters) = cache\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Recuperar dimensiones de las formas de xt y h_next\n",
        "    n_x, m =\n",
        "    n_h, m =\n",
        "\n",
        "    # Calcular derivadas relacionadas con las compuertas. Sus valores se pueden encontrar observando cuidadosamente las ecuaciones (1) a (4)\n",
        "    dot =\n",
        "    dcct =\n",
        "    dut =\n",
        "    dft =\n",
        "\n",
        "    # Calcular derivadas relacionadas con los parámetros. Usar ecuaciones (11)-(18)\n",
        "    concat_t =\n",
        "    dWf =\n",
        "    dWu =\n",
        "    dWc =\n",
        "    dWo =\n",
        "    dbf =\n",
        "    dbu =\n",
        "    dbc =\n",
        "    dbo =\n",
        "\n",
        "    # Calcular derivadas con respecto al estado oculto anterior, estado de memoria anterior y entrada. Usar ecuaciones (19)-(21).\n",
        "    dh_prev =\n",
        "    dc_prev =\n",
        "    dxt =\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Guardar gradientes en el diccionario\n",
        "    gradients = {\"dxt\": dxt, \"dh_prev\": dh_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWu\": dWu,\"dbu\": dbu,\n",
        "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "weGVc38ScqgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_cell_backward_test(lstm_cell_backward,lstm_cell_forward)"
      ],
      "metadata": {
        "id": "67lxzXQHVXk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3-4\"></a>\n",
        "### Ejercicio 6 - Backward Pass"
      ],
      "metadata": {
        "id": "-z44ukJgcv3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcular los gradientes del costo con respecto a $h^{\\langle t\\rangle}$ en cada paso de tiempo $t$ es útil porque ayuda a retropropagar el gradiente a la celda LSTM anterior. Para hacerlo, tienes que iterar a través de todos los pasos de tiempo empezando por el final $T_x$, y en cada paso, incrementar las derivadas de los parámetros y guardar $dx$.\n",
        "\n",
        "**Instrucciones**:\n",
        "-  Crea un bucle empezando desde $T_x$ retrocediendo. Para cada paso, llama tu función `lst_cell_backward` y actualiza los antiguos gradientes sumando los nuevos gradientes.\n",
        "    -  Observa que `dxt`no se actualiza, pero sí se guarda."
      ],
      "metadata": {
        "id": "t21qIqDvutzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función adaptada del curso de deepLearningAi. Lab15a: Building a Recurrent Neuronal Network Step by Step\n",
        "# Función calificadora: lstm_backward\n",
        "\n",
        "def lstm_backward(dh, caches, m_batch):\n",
        "\n",
        "    \"\"\"\n",
        "    Implementa el paso hacia atrás para la RNN con celda LSTM (sobre una secuencia completa).\n",
        "\n",
        "    Argumentos:\n",
        "    dh -- Gradientes con respecto a los estados ocultos, numpy-array de forma (n_h, m, T_x)\n",
        "    caches -- caché que almacena información del paso hacia adelante (lstm_forward)\n",
        "    m_batch -- El tamaño del minibatch, entero\n",
        "\n",
        "    Returns:\n",
        "    gradients -- diccionario de python que contiene:\n",
        "                        dx -- Gradiente de las entradas, de forma (n_x, m, T_x)\n",
        "                        dh0 -- Gradiente con respecto al estado oculto anterior, numpy array de forma (n_h, m)\n",
        "                        dWf -- Gradiente con respecto a la matriz de pesos de la compuerta de olvido, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dWu -- Gradiente con respecto a la matriz de pesos de la compuerta de actualización, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dWc -- Gradiente con respecto a la matriz de pesos de la compuerta de memoria, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dWo -- Gradiente con respecto a la matriz de pesos de la compuerta de salida, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dbf -- Gradiente con respecto a los sesgos de la compuerta de olvido, de forma (n_h, 1)\n",
        "                        dbu -- Gradiente con respecto a los sesgos de la compuerta de actualización, de forma (n_h, 1)\n",
        "                        dbc -- Gradiente con respecto a los sesgos de la compuerta de memoria, de forma (n_h, 1)\n",
        "                        dbo -- Gradiente con respecto a los sesgos de la compuerta de salida, de forma (n_h, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Recuperar valores de la primera caché de caches.\n",
        "    (caches, x) = caches\n",
        "    (h1, c1, h0, c0, f1, u1, cc1, o1, x1, parameters) = caches[0]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Recuperar dimensiones de las formas de dh y x1\n",
        "    n_h, m, T_x =\n",
        "    n_x, m =\n",
        "\n",
        "    # inicializar los gradientes con los tamaños correctos\n",
        "    dx =\n",
        "    dh0 =\n",
        "    dh_prevt =\n",
        "    dc_prevt =\n",
        "    dWf =\n",
        "    dWu =\n",
        "    dWc =\n",
        "    dWo =\n",
        "    dbf =\n",
        "    dbu =\n",
        "    dbc =\n",
        "    dbo =\n",
        "\n",
        "    # iterar hacia atrás sobre toda la secuencia\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Calcular todos los gradientes usando lstm_cell_backward.\n",
        "        gradients =\n",
        "        # Almacenar o añadir el gradiente al gradiente del paso anterior de los parámetros\n",
        "        dh_prevt =\n",
        "        dc_prevt =\n",
        "        dx[:,:,t] =\n",
        "        dWf +=\n",
        "        dWu +=\n",
        "        dWc +=\n",
        "        dWo +=\n",
        "        dbf +=\n",
        "        dbu +=\n",
        "        dbc +=\n",
        "        dbo +=\n",
        "    # Establecer el gradiente de la primera activación al gradiente retropropagado dh_prev.\n",
        "    dh0 =\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    dWf /= m_batch; dWu /= m_batch; dWc /= m_batch; dWo /= m_batch\n",
        "    dbf /= m_batch; dbu /= m_batch; dbc /= m_batch; dbo /= m_batch\n",
        "    # Almacenar los gradientes en un diccionario de python\n",
        "    gradients = {\"dx\": dx, \"dh0\": dh0, \"dWf\": dWf,\"dbf\": dbf, \"dWu\": dWu,\"dbu\": dbu,\n",
        "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "pcrgVeCucxTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_backward_test(lstm_backward,lstm_cell_backward,lstm_cell_forward)"
      ],
      "metadata": {
        "id": "nyYYOh-VV1c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 7 - Inicialización de parámetros\n",
        "\n",
        "Para la inicialización de los parámetros necesitamos obsevar que para $W_u,W_f,W_o$ y $W_c$ tenemos como función de activación la función $\\tanh$ y para la salida $W_y$ se activa con la función $\\sigma$. Esto con el objetivo de evitar problemas numéricos como la explosión o el desvanecimiento de gradientes.\n",
        "\n",
        "Para ambas funciones vamos a considerar la inicialización *Xavier* (1). Esto corresponde a tomar una distribución Gausiana con media $0$ y desviación estándar\n",
        "\n",
        "$$ \\sigma = \\sqrt{\\frac{2}{N}}\\tag{1} $$\n",
        "\n",
        "en donde $N$ es la dimensión de entrada de cada matriz de pesos $W$.\n",
        "\n",
        "Mientras que para los bias $b$ inicializaremos con vectores en blanco.\n",
        "\n",
        "**Instrucciones**:\n",
        "-  Inicializa las matrices de pesos de acuerdo a la inicialización *Xavier Glorot*.\n",
        "    -  Los parámetros `W` de las compuertas tienen forma `(n_h, n_h + n_x)`.\n",
        "    -  El parámetro `Wy` tiene la forma `(n_y, n_h)`.\n",
        "-  Inicializa los vectores de sesgo con ceros.\n",
        "    - Los parámetros `b` de las compuertas tienen la forma `(n_h,1)`.\n",
        "    - El parámetro `by` tiene la forma `(n_y,1)`.\n",
        "\n",
        "#### Pistas:\n",
        "-  Considera inicializar los pesos utilizando [np.random.randn](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html), que tiene distribución Gaussiana $\\mathcal{N}(0,1)$, y a dicho tensor aplicarle el producto con la desviación estándar."
      ],
      "metadata": {
        "id": "M7sE-rNJTNKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: initialize_LSTM_parameters\n",
        "\n",
        "def initialize_LSTM_parameters(n_h, n_x, n_y):\n",
        "  \"\"\"\n",
        "  Inicializa los parámetros de la LSTM con Xavier initialization,\n",
        "  utilizando la fórmula (1)\n",
        "\n",
        "  Arguments:\n",
        "  n_a -- número de unidades en el hidden state\n",
        "  n_x -- número de features de entrada\n",
        "  n_y -- número de features de salida\n",
        "\n",
        "  Returns:\n",
        "  parameters -- diccionario con los parámetros inicializados\n",
        "  \"\"\"\n",
        "  np.random.seed(1)\n",
        "\n",
        "  parameters = {}\n",
        "  ### START CODE HERE ###\n",
        "\n",
        "  parameters['Wf'] =\n",
        "  parameters['bf'] =\n",
        "\n",
        "  parameters['Wu'] =\n",
        "  parameters['bu'] =\n",
        "\n",
        "  parameters['Wc'] =\n",
        "  parameters['bc'] =\n",
        "\n",
        "  parameters['Wo'] =\n",
        "  parameters['bo'] =\n",
        "\n",
        "  parameters['Wy'] =\n",
        "  parameters['by'] =\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "  return parameters\n"
      ],
      "metadata": {
        "id": "t4AdTiwLsQc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initialize_LSTM_parameters_test(initialize_LSTM_parameters)"
      ],
      "metadata": {
        "id": "A3blGkjiXDX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3-5\"></a>\n",
        "### Ejercicio 8 - Entrenamiento del modelo LSTM\n",
        "Ya tienes el modelo para hacer propagación hacia adelante y retropropagación hacia atrás. Ahora, con la inicialización de parámetros, es momento de implemenetar el entrenamiento del modelo.\n",
        "\n",
        "En esta implementación vamos a utilizar el algoritmo de adam como optimizador con mini lotes -no te preocupes de esto, pues ya viene implementado y tu solo deberás programar los cálculos de la propagación y la retropropagación- y MSE como función de costo.\n",
        "\n",
        "Para este ejercicio es importante saber que nuestra función de costo se define\n",
        "\n",
        "$$\\mathcal{L} = \\frac{1}{2m_{batch}} \\sum_{i\\in batch}\\left(y_{pred} - y_{real}\\right)^2 \\tag{1}$$\n",
        "\n",
        "así, la derivada de esta función con respecto a la predicción $y_{pred}$ es de la forma\n",
        "\n",
        "$$d\\hat y =\\frac{d\\mathcal L}{d\\hat y} = \\frac{1}{m_{batch}} \\sum_{i\\in batch}\\left( y_{pred} - y_{real} \\right) \\tag{2}$$\n",
        "\n",
        "Con esto podemos calcular los parámetros de la salida y los valores del último paso del estado oculto $h^{\\langle T_x\\rangle}$ y de la célula de memoria $c^{\\langle T_x\\rangle}$. Sus derivadas son:\n",
        "\n",
        "$ d h^{\\langle T_x\\rangle} = W_y^T d \\hat y\\tag{3}$\n",
        "\n",
        "\n",
        "$ d c^{\\langle T_x\\rangle} = d \\hat y * \\Gamma_o^{\\langle T_x\\rangle} * \\left(1 - \\tanh^2\\left(c^{\\langle T_x \\rangle}\\right)\\right) \\tag{4}$\n",
        "\n",
        "Y la derivada de los parámetros de $\\hat y$ se calculan de forma similar a los parámetros de las compuertas:\n",
        "\n",
        "$$ d W_y = d\\hat y (h^{\\langle t\\rangle})^T \\tag{5}$$\n",
        "$$ d b_y = \\sum_{batches} d \\hat y \\tag{6}$$\n",
        "\n",
        "Como ahora manejamos lotes, el número de muestras está dado por `m_batch`.\n",
        "\n",
        "**Instrucciones**:\n",
        "-   Obtén los valores del estado oculto `h`, la predicción `y_pred`, célula de memoria `c` y el cache `cache`, calculando la propagación hacia adelante.\n",
        "    - El estado oculto `h`tiene la forma `(n_h,m_batch,T_x)`.\n",
        "    - La predicción `y_pred` tiene la forma `(n_y,m_batch)`.\n",
        "    - La célula de memoria `c`tiene la forma `(n_h,m_batch,T_x)`.\n",
        "-   Calcula el error de predicción llamando la función `compute_cost()` con las variables `y_pred` y `minibatch_Y`.\n",
        "    - La variable del mini lote `minibatch_Y` tiene forma `(n_y,m_batch,T_x)`.\n",
        "-   Inicializa las derivadas $d h$ y $dc$ en las variables `dh` y `dc`, respectivamente.\n",
        "    - Ambas variables tienen la forma `(n_h,m_batch,T_x)`.\n",
        "-   Calcula la derivada de la función de costo utilizando la ecuación (2) en la variable `dcost_dy_pred`.\n",
        "-   Calcula la derivada del estado oculto $h^{\\langle T_x\\rangle}$ y de la célula de memoria $c^{\\langle T_x\\rangle}$ utilizando las ecuaciones (3) y (4), respectivamente.\n",
        "    -   Accede al paso $T_x$ con `h[:,:,-1]` y `c[:,:,-1]`.\n",
        "    -   Los valores de la propagación se han extraído anteriormente.\n",
        "-   Obtén los gradientes calculando la retropropagación.\n",
        "-   Actualiza el valor de las derivadas de los parámetros $W_y$ y $b_y$ utilizando las ecuaciones (5) y (6).\n",
        "\n",
        "#### Pistas:\n",
        "-  La función [np.zeros_like](https://numpy.org/devdocs/reference/generated/numpy.zeros_like.html) puede ser útil para inicializar los tensores.\n",
        "-  Utiliza la función [np.squeeze](https://numpy.org/devdocs/reference/generated/numpy.squeeze.html) para igualar dimensiones entre las variables `y_pred` y `minibatch_Y`."
      ],
      "metadata": {
        "id": "Plh4MUolybQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: model_lstm_adam\n",
        "\n",
        "def model_lstm_adam(X, Y, n_h, learning_rate=0.001, mini_batch_size=32,\n",
        "                    beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
        "                    num_epochs=1000, print_cost=True):\n",
        "  \"\"\"\n",
        "  Entrena una LSTM desde cero con Adam y minibatches.\n",
        "\n",
        "  Arguments:\n",
        "  X -- datos de entrada, numpy array (n_x, m, T_x)\n",
        "  Y -- datos objetivo, numpy array (n_y, m, 1)\n",
        "  n_a -- número de unidades del estado oculto\n",
        "  learning_rate -- tasa de aprendizaje Adam\n",
        "  mini_batch_size -- tamaño de minibatch\n",
        "  num_epochs -- número de épocas\n",
        "  \"\"\"\n",
        "\n",
        "  n_x, m, T_x = X.shape\n",
        "  n_y, _, _ = Y.shape\n",
        "\n",
        "  # Inicialización\n",
        "  parameters = initialize_LSTM_parameters(n_h, n_x, n_y)\n",
        "  v, s = initialize_adam(parameters)\n",
        "  seed = 10\n",
        "  costs = []\n",
        "  t = 0  # contador de pasos de Adam\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    seed += 1\n",
        "    minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
        "    epoch_cost = 0\n",
        "\n",
        "    for minibatch_X, minibatch_Y in minibatches:\n",
        "      # Inicialización del estado oculto\n",
        "      m_batch = minibatch_X.shape[1]\n",
        "      h0 = np.zeros((n_h, m_batch))\n",
        "\n",
        "      ### START CODE HERE ###\n",
        "      # Forward pass utilizando tu función implementada anteriormente lstm_forward(minibatch_X, h0, parameters)\n",
        "      h, y_pred, c, caches =\n",
        "\n",
        "      # Obtenemos los valores del cache en el último instante\n",
        "      caches_list = caches[0] if isinstance(caches, tuple) else caches\n",
        "      _, c_next_last, _, _, _, _, _, ot_last, _, _ = caches_list[-1]\n",
        "\n",
        "      # Costo (MSE) - Calcula y_pred (n_y, m) con minibatch_Y (n_y, m, 1), utiliza .squeeze(axis=2) para igualar dimensiones.\n",
        "      cost =\n",
        "\n",
        "      # Inicializa las derivadas del estado oculto y de la celula de memoria.\n",
        "      dh = # Forma (n_h, m_batch, T_x)\n",
        "      dc = # Forma (n_h, m_batch, T_x)\n",
        "\n",
        "      # Calcula la derivada de la predicción de acuerdo a la ecuación (2). Recuerda utilizar .squeeze(axis=2) para igualar dimensiones.\n",
        "      dcost_dy_pred =\n",
        "\n",
        "      # Propaga la derivada al último bloque, utiliza las ecuaciones (3)-(4).\n",
        "      dh[:, :, -1] =\n",
        "      dc[:, :, -1] =\n",
        "\n",
        "      # Backward pass utilizando tu función implementada anteriormente lstm_backward(dh, caches, m_batch).\n",
        "      grads =\n",
        "\n",
        "      # Calcula las derivadas de los parámetros de la salida, utiliza las ecuaciones (5)-(6).\n",
        "      grads['dWy'] =\n",
        "      grads['dby'] =\n",
        "\n",
        "      ### END CODE HERE ###\n",
        "\n",
        "      # Recortar gradientes\n",
        "      grads = clip_gradients(grads, 5.0)\n",
        "      # Update Adam\n",
        "      t += 1\n",
        "      parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
        "      epoch_cost += cost / len(minibatches)\n",
        "\n",
        "    # Guardar costo\n",
        "    if print_cost:\n",
        "      print(f\"Cost after epoch {epoch}: {epoch_cost:.6f}\")\n",
        "      costs.append(epoch_cost)\n",
        "\n",
        "  # Graficar costos\n",
        "  if print_cost:\n",
        "    plot_training(costs)\n",
        "\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "NKPxMVmGv3-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm_adam_test(model_lstm_adam)"
      ],
      "metadata": {
        "id": "DmAwaTm7XHlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecuta la siguiente celda para entrenar tu primer modelo LSTM. Puedes experimentar con los diversos hiperparámetros"
      ],
      "metadata": {
        "id": "jUSw9vCxtOJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modeloLSTM = model_lstm_adam(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    n_h= 78,                # Número de unidades del estado oculto\n",
        "    learning_rate=0.001,    # Tasa de aprendizaje\n",
        "    mini_batch_size=64,     # Tamaño del minibatch\n",
        "    num_epochs=200,         # Número de épocas\n",
        "    print_cost=True         # Mostrar el costo\n",
        ")\n"
      ],
      "metadata": {
        "id": "AQG24suI9v2h",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"3-6\"></a>\n",
        "### Ejercicio 9 - Predicción del modelo LSTM\n",
        "Listo, ya haz entrenado tu modelo LSTM. Sin embargo, ahora queremos hacer predicciones con este. Tu función `lstm_forward` realiza una sola predicción sobre el número $T_x$ de pasos. Sin embargo, si queremos realizar predicciones sobre más instantes de tiempo, debemos actualizar las ventanas de tiempo y los valores de las ventanas con los valores predichos.\n",
        "\n",
        "Para esto nuestra función `predict_lstm` recibe como parámetros la ventana actual `window`, el modelo `parameters` y el número de pasos `steps`. Nosotros extraemos los valores de las dimensiones necesarios e inicializamos el estado oculto inicial $h^{\\langle 0\\rangle}$ y una lista con los valores a predecir `future_LSTM_predictions`. Tu objetivo es implementar el bucle principal.\n",
        "\n",
        "**Instrucciones**:\n",
        "-  Ajusta el número de iteraciones en el bucle principal.\n",
        "-  Realiza la predicción $\\hat y$ utilizando la función `lstm_forward`.\n",
        "    - Recuerda que la función `lstm_forward` retorna una tupla `(h,y,c,caches)`, en este caso solo nos interesa el valor `y`.\n",
        "-  Guarda la predicción en la lista `future_LSTM_predictions`.\n",
        "    - Aplana el tensor 2D para guardar un solo valor.\n",
        "-  Actualiza una nueva ventana con el valor que acabas de predecir $\\hat y$.\n",
        "-  Actualiza la ventana redimensionándo la nueva ventana para tener forma `(1, 1, T_x)`."
      ],
      "metadata": {
        "id": "vAN7gS1Myox7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: predict_lstm\n",
        "\n",
        "def predict_lstm(window, parameters, steps):\n",
        "  \"\"\"\n",
        "  Realiza un número de predicciones con el modelo LSTM empezando desde una ventana dada.\n",
        "\n",
        "  Argumentos:\n",
        "  window -- Ventana de entrada inicial para la predicción, shape (n_x, 1, T_x)\n",
        "  parameters -- Diccionario de Python con los parámetros entrenados (Wf, bf, Wu, bu, ...)\n",
        "  steps -- Número de pasos de predicción a realizar\n",
        "\n",
        "  Retorna:\n",
        "  future_LSTM_predictions -- Una lista con las predicciones.\n",
        "  \"\"\"\n",
        "  # Extraer dimensiones de la entrada.\n",
        "  n_x, m, T_x = window.shape\n",
        "  n_y, n_h = parameters[\"Wy\"].shape\n",
        "\n",
        "  # Lista para guardar las predicciones.\n",
        "  future_LSTM_predictions_scaled = []\n",
        "  # Inicializa el primer estado oculto con ceros.\n",
        "  h0 = np.zeros((n_h, 1))\n",
        "\n",
        "  ## START CODE HERE ##\n",
        "  # Itera hasta el número de pasos dado en el parámetro steps.\n",
        "  for _ in range( ??? ):\n",
        "    # Realiza la prediccion sobre el conjunto de datos y de parámetros.\n",
        "    h, y, c, caches =\n",
        "\n",
        "    # Guarda la prediccion, aplana el tensor 2D utilizando .flatten().\n",
        "    future_LSTM_predictions_scaled.append( ??? )\n",
        "\n",
        "    # Actualiza los valores de la ventana. Aplana el tensor de la ventana primero.\n",
        "    new_window_values = np.append( ??? )\n",
        "\n",
        "    # Redimensiona la ventana actual para mantener la forma (1,1,T_x).\n",
        "    window = new_window_values.reshape( ??? )\n",
        "  ## END CODE HERE ##\n",
        "\n",
        "  #Desnormaliza los datos\n",
        "  future_LSTM_predictions = scaler.inverse_transform(np.array(future_LSTM_predictions_scaled).reshape(-1, 1))\n",
        "\n",
        "  return future_LSTM_predictions.flatten()"
      ],
      "metadata": {
        "id": "cP8KJSOca1aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_lstm_test(predict_lstm)"
      ],
      "metadata": {
        "id": "F0-rYEGJXhIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicción\n",
        "Ahora veamos la predicción del modelo para los doce meses próximos del dataset."
      ],
      "metadata": {
        "id": "4ysGFRvNcmH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tomamos la última ventana de datos conocida del conjunto de prueba\n",
        "current_window = X_test[:, -1:, :]\n",
        "test_LSTM_prediction = predict_lstm(current_window, modeloLSTM, 24)\n",
        "\n",
        "print(\"Predicciones de inflación para los próximos 24 meses:\")\n",
        "print(test_LSTM_prediction)"
      ],
      "metadata": {
        "id": "eIMsQO-Zcwhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Des-normalizar los datos para la gráfica\n",
        "y_train_actual = scaler.inverse_transform(y_train_keras.reshape(-1, 1))\n",
        "y_test_actual = scaler.inverse_transform(y_test_keras.reshape(-1, 1))\n",
        "\n",
        "# Crea un índice de fechas para la gráfica\n",
        "# Los índices de las secuencias corresponden a la *última* fecha de cada ventana.\n",
        "# Por lo tanto, y_train[i] corresponde a data.index[i + n_steps_in].\n",
        "start_train_index = n_steps_in\n",
        "end_train_index = start_train_index + train_size\n",
        "train_dates = data.index[start_train_index:end_train_index]\n",
        "\n",
        "start_test_index = end_train_index\n",
        "end_test_index = start_test_index + len(y_test_keras)\n",
        "test_dates = data.index[start_test_index:end_test_index]\n",
        "# Crear fechas futuras para las predicciones\n",
        "# La primera fecha futura es el mes siguiente a la última fecha de prueba\n",
        "future_dates = pd.date_range(start=test_dates[-1], periods=25, freq='ME')[1:]\n",
        "# Graficar los resultados\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Datos originales\n",
        "plt.plot(train_dates, y_train_actual, label='Datos de Entrenamiento (Reales)', color='blue')\n",
        "plt.plot(test_dates, y_test_actual, label='Datos de Prueba (Reales)', color='green')\n",
        "\n",
        "# Predicciones\n",
        "# future_predictions ya está desnormalizado\n",
        "plt.plot(future_dates, test_LSTM_prediction, label='Predicción a 24 Meses', color='red', linestyle='--')\n",
        "\n",
        "# Estilo del gráfico\n",
        "plt.title('Predicción de Inflación con LSTM', fontsize=16)\n",
        "plt.xlabel('Fecha', fontsize=12)\n",
        "plt.ylabel('Valor de Inflación', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C7eYqt7rdJhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"4\"></a>\n",
        "## Segundo Modelo: Construyendo una GRU\n",
        "\n",
        "La arquitectura de la celda GRU puede verse en la siguiente Figura.\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/MateoOrtiz001/Laboratory-LSTM-GRU-Inflation/main/imagenes/GRU01.png\" width=\"500\">\n",
        "</center>\n",
        "La GRU (Gated Recurrent Unit) es una versión simplificada de la LSTM. Se tienen dos compuertas $\\Gamma_r$ y $\\Gamma_u$, un estado oculto $h^{\\langle t\\rangle}$ y un candidato al estado oculto $\\tilde{h}^{\\langle t\\rangle}$.\n",
        "\n",
        "-  *Compuerta de actualización* $\\Gamma_u$: es un tensor que contiene valores entre $0$ y $1$.\n",
        "    - Determina qué tanto del estado oculto anterior debe ser conservado y qué tanto del nuevo candidato debe ser incorporado.\n",
        "    - Valores cercanos a $0$ indican que se debe conservar principalmente el estado anterior.\n",
        "    - Valores cercanos a $1$ indican que se debe incorporar principalmente el nuevo candidato.\n",
        "\n",
        "    $$\\Gamma_u = \\sigma \\left(W_u \\left[h^{\\langle t-1\\rangle}; x^{\\langle t\\rangle}\\right] + b_u\\right)  \\tag{1}$$\n",
        "\n",
        "-  *Compuerta de relevancia* (o reset) $\\Gamma_r$: es un tensor que contiene valores entre $0$ y $1$.\n",
        "    - Determina qué tanto del estado oculto anterior es relevante para calcular el nuevo candidato.\n",
        "    - Valores cercanos a $0$ indican que el estado anterior debe ser \"olvidado\" al calcular el candidato.\n",
        "    - Valores cercanos a $1$ indican que el estado anterior es altamente relevante.\n",
        "\n",
        "    $$\\Gamma_r = \\sigma \\left(W_r \\left[h^{\\langle t-1\\rangle}; x^{\\langle t\\rangle}\\right] + b_r\\right)  \\tag{2}$$\n",
        "\n",
        "-  *Estado oculto candidato* $\\tilde{h}^{\\langle t\\rangle}$: es un tensor que contiene la nueva información que podría ser incorporada al estado oculto.\n",
        "    - Como los datos están normalizados, este tensor contiene valores entre $-1$ y $1$.\n",
        "    - La compuerta de relevancia $\\Gamma_r$ modula cuánto del estado anterior se usa para calcular este candidato.\n",
        "\n",
        "    $$\\tilde{h}^{\\langle t\\rangle} = \\tanh\\left(W_h \\left[\\Gamma_r * h^{\\langle t-1\\rangle}; x^{\\langle t\\rangle}\\right] + b_h\\right) \\tag{3} $$\n",
        "\n",
        "-  *Estado oculto* $h^{\\langle t\\rangle}$: es un tensor que sirve como entrada para el siguiente paso de tiempo.\n",
        "    - Es usado para determinar las compuertas del siguiente paso del tiempo.\n",
        "    - Es una combinación lineal entre el estado oculto anterior y el candidato, ponderada por la compuerta de actualización.\n",
        "    - Cuando $\\Gamma_u \\approx 0$, se conserva principalmente $h^{\\langle t-1\\rangle}$.\n",
        "    - Cuando $\\Gamma_u \\approx 1$, se incorpora principalmente $\\tilde{h}^{\\langle t\\rangle}$.\n",
        "\n",
        "    $$h^{\\langle t\\rangle} = \\Gamma_u * \\tilde{h}^{\\langle t\\rangle} + (1 - \\Gamma_u) * h^{\\langle t-1\\rangle} \\tag{4}$$"
      ],
      "metadata": {
        "id": "bEc2WL5gYE4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"4-1\"></a>\n",
        "### Ejercicio 10 - Celda del Forward Pass\n",
        "Implementa la celda GRU observada en la Figura.\n",
        "\n",
        "**Instrucciones**:\n",
        "1. Concatena el estado oculto $h^{\\langle t-1 \\rangle}$ y $x^{\\langle t \\rangle}$ en una sola matriz:  \n",
        "\n",
        "$$concat = \\begin{bmatrix} h^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$$  \n",
        "\n",
        "2. Computa las fórmulas (1) y (2) para las compuertas $\\Gamma_u$ y $\\Gamma_r$.\n",
        "\n",
        "3. Concatena el estado oculto modulado $\\Gamma_r * h^{\\langle t-1 \\rangle}$ y $x^{\\langle t \\rangle}$ para calcular el candidato:\n",
        "\n",
        "$$concat\\_candidate = \\begin{bmatrix} \\Gamma_r * h^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$$\n",
        "\n",
        "4. Computa las fórmulas (3) y (4) para el estado oculto candidato y el estado oculto final.\n",
        "\n",
        "5. Para este problema, no tienes que computar la predicción $y^{\\langle t \\rangle}$ en todas las celdas, luego no deberás implementarla aquí.\n",
        "\n",
        "#### Pistas\n",
        "* Puedes usar [numpy.concatenate](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html).  Revisa qué valor usar para el parámetro `axis`.\n",
        "* Las funciones `sigmoid()` y `softmax` se importan desde `utils.py`.\n",
        "* La documentación sobre [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html)\n",
        "* Usa [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) para la multiplicación de matrices.\n"
      ],
      "metadata": {
        "id": "MCAI5KUOy8Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: gru_cell_forward\n",
        "\n",
        "def gru_cell_forward(xt, h_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implementa un solo paso hacia adelante de la celda GRU como se describe en la Figura (4)\n",
        "\n",
        "    Argumentos:\n",
        "    xt -- tus datos de entrada en el paso de tiempo \"t\", numpy array de forma (n_x, m).\n",
        "    h_prev -- Estado oculto en el paso de tiempo \"t-1\", numpy array de forma (n_h, m)\n",
        "    parameters -- diccionario de python que contiene:\n",
        "                        Wu -- Matriz de pesos de la compuerta de actualización, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bu -- Sesgo de la compuerta de actualización, numpy array de forma (n_h, 1)\n",
        "                        Wh -- Matriz de pesos del valor candidato, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bh --  Sesgo del valor candidato, numpy array de forma (n_h, 1)\n",
        "                        Wr -- Matriz de pesos de la compuerta de reinicio, numpy array de forma (n_h, n_h + n_x)\n",
        "                        br -- Sesgo de la compuerta de reinicio, numpy array de forma (n_h, 1)\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    h_next -- siguiente estado oculto, de forma (n_h, m)\n",
        "    cache -- tupla de valores necesarios para el paso hacia atrás, contiene (h_next, h_prev, ut, rt, hht, xt, parameters)\n",
        "\n",
        "    Nota: ut/rt significan las compuertas de actualización/reinicio, hh significa el valor candidato (h tilde).\n",
        "    \"\"\"\n",
        "\n",
        "    # Recuperar parámetros de \"parameters\"\n",
        "    Wu = parameters[\"Wu\"]\n",
        "    bu = parameters[\"bu\"]\n",
        "    Wh = parameters[\"Wh\"]\n",
        "    bh = parameters[\"bh\"]\n",
        "    Wr = parameters[\"Wr\"]\n",
        "    br = parameters[\"br\"]\n",
        "\n",
        "\n",
        "    # Recuperar dimensiones de las formas de xt\n",
        "    n_x, m = xt.shape\n",
        "    n_h, _ = h_prev.shape # Asumiendo que la forma de h_prev es (n_h, m)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Concatenar h_prev y xt\n",
        "    concat =\n",
        "\n",
        "    # Calcular valores para ut, rt, hht, h_next usando las fórmulas dadas en la figura (4)\n",
        "    ut =\n",
        "    rt =\n",
        "    ht =\n",
        "    concat_candate =\n",
        "    hht =\n",
        "    h_next =\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # almacenar valores necesarios para la propagación hacia atrás en la caché\n",
        "    cache = (h_next, h_prev, ut, rt, hht, xt, parameters)\n",
        "\n",
        "    return h_next, cache"
      ],
      "metadata": {
        "id": "5DWZumQoYhGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_cell_forward_test(gru_cell_forward)"
      ],
      "metadata": {
        "id": "A671jsCpXlgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"4-2\"></a>\n",
        "### Ejercicio 11 - Forward Pass\n",
        "Ya has implementado un paso de una GRU, ahora puedes iterar sobre toda la secuencia utilizando un bucle sobre todas las entradas $T_x$.\n",
        "\n",
        "Implementa el modelo GRU iterando las celdas hasta los $T_x$ pasos de tiempo.\n",
        "\n",
        "**Instrucciones**:\n",
        "-  Obtén las dimensiones $n_x,n_h,n_y,m,T_x$ de las formas de las variables `x` y `parameters`.\n",
        "-  Inicializa los tensores 3D $h$ y $y$\n",
        "    -  $h$: estado oculto, forma $(n_h,m,T_x)$.\n",
        "    -  $y$: predicción, forma $(n_h,m,T_y)$ donde $T_y=1$.\n",
        "-  Inicializa el tensor 2D $h^{\\langle t\\rangle}$.\n",
        "    -  $h^{\\langle t\\rangle}$ guarda el estado oculto para el paso de tiempo $t$. El nombre de la variable es `h_next`.\n",
        "    -  $h^{\\langle 0\\rangle}$, el estado oculto en el tiempo $0$, se pasa cuando se llama la función. El nombre de la variable es `h0`.\n",
        "    -  Inicializa $h^{\\langle t\\rangle}$ con el estado oculto $h^{\\langle 0\\rangle}$ que se pasa a la función.\n",
        "-  Para cada instante de tiempo, realiza lo siguiente\n",
        "    -  Desde el tensor 3D $x$, toma la parte 2D $x^{\\langle t\\rangle}$ en el instante de tiempo $t$.\n",
        "    -  Llama la función `gru_cell_forward` definida anteriormente, para obtener el estado oculto y el cache.\n",
        "    -  Guarda el estado oculto dentro del tensor 3D.\n",
        "    -  Agrega el cache a los caches.\n",
        "\n",
        "*Nota*: En este caso observa que la predicción se retorna directamente como un tensor 2D."
      ],
      "metadata": {
        "id": "-AyEW2H3zgiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: gru_forward\n",
        "\n",
        "def gru_forward(x, h0, parameters):\n",
        "    \"\"\"\n",
        "    Implementa la propagación hacia adelante de la red neuronal recurrente usando una celda GRU descrita en la Figura (4).\n",
        "\n",
        "    Argumentos:\n",
        "    x -- Datos de entrada para cada paso de tiempo, de forma (n_x, m, T_x).\n",
        "    h0 -- Estado oculto inicial, de forma (n_h, m)\n",
        "    parameters -- diccionario de python que contiene:\n",
        "                        Wu -- Matriz de pesos de la compuerta de actualización, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bu -- Sesgo de la compuerta de actualización, numpy array de forma (n_h, 1)\n",
        "                        Wh -- Matriz de pesos del valor candidato, numpy array de forma (n_h, n_h + n_x)\n",
        "                        bh -- Sesgo del valor candidato, numpy array de forma (n_h, 1)\n",
        "                        Wr -- Matriz de pesos de la compuerta de reinicio, numpy array de forma (n_h, n_h + n_x)\n",
        "                        br -- Sesgo de la compuerta de reinicio, numpy array de forma (n_h, 1)\n",
        "                        Wy -- Matriz de pesos que relaciona el estado oculto con la salida, numpy array de forma (n_y, n_h)\n",
        "                        by -- Sesgo que relaciona el estado oculto con la salida, numpy array de forma (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    h -- Estados ocultos para cada paso de tiempo, numpy array de forma (n_h, m, T_x)\n",
        "    y -- Predicciones para cada paso de tiempo, numpy array de forma (n_y, m)\n",
        "    caches -- tupla de valores necesarios para el paso hacia atrás, contiene (lista de todas las cachés, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializar \"caches\", que rastreará la lista de todas las cachés\n",
        "    caches = []\n",
        "\n",
        "    Wy = parameters['Wy']\n",
        "    by = parameters['by']\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Recuperar dimensiones de las formas de x y parameters['Wy']\n",
        "    n_x, m, T_x =\n",
        "    n_y, n_h =\n",
        "\n",
        "    # inicializar \"h\" y \"y\" con ceros\n",
        "    h =\n",
        "    y =\n",
        "\n",
        "    # Inicializar h_next\n",
        "    h_next =\n",
        "\n",
        "    # iterar sobre todos los pasos de tiempo\n",
        "    for t in range(T_x): # Iterar a través de todos los pasos de tiempo\n",
        "        # Obtener la sección 2D 'xt' de la entrada 3D 'x' en el paso de tiempo 't'\n",
        "        xt =\n",
        "        # Actualizar el siguiente estado oculto, calcular la predicción, obtener la caché\n",
        "        h_next, cache =\n",
        "\n",
        "        # Guardar el valor del nuevo estado oculto \"siguiente\" en h\n",
        "        h[:,:,t] =\n",
        "        # Añadir la caché a caches\n",
        "        caches.append( ??? )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    y = np.dot(Wy,h[:,:,-1]) + by\n",
        "    # almacenar valores necesarios para la propagación hacia atrás en la caché\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return h, y, caches"
      ],
      "metadata": {
        "id": "4a85wa3FY2cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_cell_forward_test(gru_cell_forward)"
      ],
      "metadata": {
        "id": "rnClVO2JXnnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"4-3\"></a>\n",
        "### Ejercicio 12 - Celda del Backward pass\n",
        "Ya con la propagación hacia adelante programada, ahora podemos calcular la retropropagación hacia atrás para el entrenamiento del modelo GRU.\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/MateoOrtiz001/Laboratory-LSTM-GRU-Inflation/main/imagenes/GRU02.png\" width=\"500\">\n",
        "</center>\n",
        "En cada celda calculamos la derivada de las variables asociadas a los pesos y sesgos de las compuertas, y las derivadas de la entrada $x^{\\langle t\\rangle}$ y del estado anterior $h^{\\langle t-1\\rangle}$.\n",
        "\n",
        "Como la retropropagación se hace hacia atrás de forma iterativa desde el instante $T_x$, en cada paso tenemos $dh^{\\langle t\\rangle}$, llamado en el código `dh_next`. Con esto, las derivadas de cada compuerta está dada como sigue.\n",
        "\n",
        "\\begin{align}\n",
        "d\\tilde{h}^{\\langle t \\rangle} &= dh^{\\langle t \\rangle} * \\Gamma_u^{\\langle t \\rangle} * \\left(1-\\left(\\tilde{h}^{\\langle t \\rangle}\\right)^2\\right)\\tag{1} \\\\[8pt]\n",
        "du^{\\langle t \\rangle} &= \\left(dh^{\\langle t\\rangle} * \\tilde{h}^{\\langle t \\rangle} - dh^{\\langle t\\rangle} * h^{\\langle t-1\\rangle}\\right) * \\Gamma_u^{\\langle t \\rangle} * \\left(1-\\Gamma_u^{\\langle t \\rangle}\\right)\\tag{2} \\\\[8pt]\n",
        "dr^{\\langle t \\rangle} &= \\left(W_h^T d\\tilde{h}^{\\langle t \\rangle} * h^{\\langle t-1\\rangle}\\right) * \\Gamma_r^{\\langle t \\rangle} * \\left(1-\\Gamma_r^{\\langle t \\rangle}\\right)\\tag{3}\n",
        "\\end{align}\n",
        "\n",
        "Con esto podemos calcular las derivadas de los parámetros. Primero, la derivada de los pesos está dada como\n",
        "\n",
        "$ dW_r = dr^{\\langle t \\rangle} \\begin{bmatrix} h^{\\langle t-1\\rangle} \\\\ x^{\\langle t\\rangle}\\end{bmatrix}^T \\tag{4} $\n",
        "\n",
        "$ dW_u = du^{\\langle t \\rangle} \\begin{bmatrix} h^{\\langle t-1\\rangle} \\\\ x^{\\langle t\\rangle}\\end{bmatrix}^T \\tag{5} $\n",
        "\n",
        "$ dW_h = d\\tilde{h}^{\\langle t \\rangle} \\begin{bmatrix} \\Gamma_r^{\\langle t\\rangle} * h^{\\langle t-1\\rangle} \\\\ x^{\\langle t\\rangle}\\end{bmatrix}^T \\tag{6}$\n",
        "\n",
        "Para calcular los sesgos $db_r, db_u, db_h$ solo necesitas sumar a través de todas las 'm' muestras (axis= 1) de $dr^{\\langle t \\rangle}, du^{\\langle t \\rangle}, d\\tilde{h}^{\\langle t \\rangle}$ respectivamente. Observa que tienes que tener `keepdims = True`.\n",
        "\n",
        "$\\displaystyle db_r = \\sum_{batch}dr^{\\langle t \\rangle}\\tag{7}$\n",
        "$\\displaystyle db_u = \\sum_{batch}du^{\\langle t \\rangle}\\tag{8}$\n",
        "$\\displaystyle db_h = \\sum_{batch}d\\tilde{h}^{\\langle t \\rangle}\\tag{9}$\n",
        "\n",
        "Finalmente, debes calcular la derivada con respecto al estado anterior y la entrada.\n",
        "\n",
        "$ dh^{\\langle t-1\\rangle} = W_r^T dr^{\\langle t \\rangle} + W_u^T du^{\\langle t \\rangle} + W_h^T d\\tilde{h}^{\\langle t \\rangle} * \\Gamma_r^{\\langle t \\rangle} + dh^{\\langle t\\rangle} * (1 - \\Gamma_u^{\\langle t \\rangle}) \\tag{10}$\n",
        "\n",
        "Aquí, para referirnos a la concatenación de los pesos para la ecuación (10) son los primeros $n_h$ valores, es decir, $W_r = W_r[:,:n_h]$, etc.\n",
        "\n",
        "$ dx^{\\langle t \\rangle} = W_r^T dr^{\\langle t \\rangle} + W_u^T du^{\\langle t \\rangle} + W_h^T d\\tilde{h}^{\\langle t \\rangle}\\tag{11} $\n",
        "\n",
        "en donde los pesos para la ecuación (11) van desde $n_h$ hasta el final, es decir $W_r = W_r[:,n_h:]$, etc.\n",
        "\n",
        "**Instrucciones:**\n",
        "-  Implementa las ecuaciones (1) - (11) de arriba.\n",
        "    - $d\\tilde{h}^{\\langle t \\rangle}$ es representado por `dhht`,    \n",
        "    - $du^{\\langle t \\rangle}$ es representado por `dut`,  \n",
        "    - $dr^{\\langle t \\rangle}$ es representado por `drt`."
      ],
      "metadata": {
        "id": "0F-MfqUtzmdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: gru_cell_backward\n",
        "\n",
        "def gru_cell_backward(dh_next, cache):\n",
        "    \"\"\"\n",
        "    Implementa el paso hacia atrás para la celda GRU (un solo paso de tiempo).\n",
        "\n",
        "    Argumentos:\n",
        "    dh_next -- Gradientes del siguiente estado oculto, de forma (n_h, m)\n",
        "    cache -- caché que almacena información del paso hacia adelante\n",
        "\n",
        "    Returns:\n",
        "    gradients -- diccionario de python que contiene:\n",
        "                        dxt -- Gradiente de los datos de entrada en el paso de tiempo t, de forma (n_x, m)\n",
        "                        dh_prev -- Gradiente con respecto al estado oculto anterior, numpy array de forma (n_h, m)\n",
        "                        dWr -- Gradiente con respecto a la matriz de pesos de la compuerta de reinicio, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dbr -- Gradiente con respecto a los sesgos de la compuerta de reinicio, de forma (n_h, 1)\n",
        "                        dWu -- Gradiente con respecto a la matriz de pesos de la compuerta de actualización, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dbu -- Gradiente con respecto a los sesgos de la compuerta de actualización, de forma (n_h, 1)\n",
        "                        dWh -- Gradiente con respecto a la matriz de pesos del estado oculto candidato, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dbh -- Gradiente con respecto a los sesgos del estado oculto candidato, de forma (n_h, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Recuperar información de \"cache\"\n",
        "    (h_next, h_prev, ut, rt, hht, xt, parameters) = cache\n",
        "\n",
        "    Wh = parameters['Wh']\n",
        "    Wu = parameters['Wu']\n",
        "    Wr = parameters['Wr']\n",
        "    concat_c = np.concatenate((rt*h_prev,xt),axis=0).T\n",
        "    ### START CODE HERE ###\n",
        "    # Recuperar dimensiones de las formas de xt y h_next\n",
        "    n_x, m =\n",
        "    n_h, m =\n",
        "\n",
        "    # Calcular derivadas relacionadas con las compuertas. Sus valores se pueden encontrar observando cuidadosamente las ecuaciones (7) a (10)\n",
        "    dhht =\n",
        "    dut =\n",
        "    drt =\n",
        "\n",
        "\n",
        "    # Calcular derivadas relacionadas con los parámetros. Usar ecuaciones (11)-(18)\n",
        "    concat_t =\n",
        "    dWr =\n",
        "    dWu =\n",
        "    dWh =\n",
        "    dbr =\n",
        "    dbu =\n",
        "    dbh =\n",
        "\n",
        "    # Calcular derivadas con respecto al estado oculto anterior y entrada. Usar ecuaciones (19)-(21).\n",
        "    dh_prev =\n",
        "    dxt =\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Guardar gradientes en el diccionario\n",
        "    gradients = {\"dxt\": dxt, \"dh_prev\": dh_prev, \"dWr\": dWr,\"dbr\": dbr, \"dWu\": dWu,\"dbu\": dbu,\n",
        "                 \"dWh\": dWh, \"dbh\": dbh}\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "FJ01F4V-ZD9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_cell_backward_test(gru_cell_backward,gru_cell_forward)"
      ],
      "metadata": {
        "id": "p42Cby5rXsrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"4-4\"></a>\n",
        "### Ejercicio 13 - Backward pass\n",
        "Calcular los gradientes del costo con respecto a $h^{\\langle t\\rangle}$ en cada paso de tiempo $t$ es útil porque ayuda a retropropagar el gradiente a la celda LSTM anterior. Para hacerlo, tienes que iterar a través de todos los pasos de tiempo empezando por el final $T_x$, y en cada paso, incrementar las derivadas de los parámetros y guardar $dx$.\n",
        "\n",
        "**Instrucciones**:\n",
        "-  Crea un bucle empezando desde $T_x$ retrocediendo. Para cada paso, llama tu función `gru_cell_backward` y actualiza los antiguos gradientes sumando los nuevos gradientes.\n",
        "    -  Observa que `dxt`no se actualiza, pero sí se guarda."
      ],
      "metadata": {
        "id": "i6KaQiKfzvMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: gru_backward\n",
        "\n",
        "def gru_backward(dh, caches, m_batch):\n",
        "\n",
        "    \"\"\"\n",
        "    Implementa el paso hacia atrás para la RNN con celda GRU (sobre una secuencia completa).\n",
        "\n",
        "    Argumentos:\n",
        "    dh -- Gradientes con respecto a los estados ocultos, numpy-array de forma (n_h, m, T_x)\n",
        "    caches -- caché que almacena información del paso hacia adelante (gru_forward)\n",
        "    m_batch -- El tamaño del minibatch, entero\n",
        "\n",
        "    Returns:\n",
        "    gradients -- diccionario de python que contiene:\n",
        "                        dx -- Gradiente de las entradas, de forma (n_x, m, T_x)\n",
        "                        dh0 -- Gradiente con respecto al estado oculto anterior, numpy array de forma (n_h, m)\n",
        "                        dWr -- Gradiente con respecto a la matriz de pesos de la compuerta de reinicio, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dWu -- Gradiente con respecto a la matriz de pesos de la compuerta de actualización, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dWc -- Gradiente con respecto a la matriz de pesos del valor candidato, numpy array de forma (n_h, n_h + n_x)\n",
        "                        dbr -- Gradiente con respecto a los sesgos de la compuerta de reinicio, de forma (n_h, 1)\n",
        "                        dbu -- Gradiente con respecto a los sesgos de la compuerta de actualización, de forma (n_h, 1)\n",
        "                        dbc -- Gradiente con respecto a los sesgos del valor candidato, de forma (n_h, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Recuperar valores de la primera caché (t=1) de caches.\n",
        "    (caches, x) = caches\n",
        "    (h1, h0, u1, r1, cc1, x1, parameters) = caches[0]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Recuperar dimensiones de las formas de dh y x1\n",
        "    n_h, m, T_x =\n",
        "    n_x, m =\n",
        "\n",
        "    # inicializar los gradientes con los tamaños correctos\n",
        "    dx =\n",
        "    dh0 =\n",
        "    dh_prevt =\n",
        "    dWu =\n",
        "    dWr =\n",
        "    dWh =\n",
        "    dbu =\n",
        "    dbr =\n",
        "    dbh =\n",
        "\n",
        "\n",
        "    # iterar hacia atrás sobre toda la secuencia\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Calcular todos los gradientes usando gru_cell_backward.\n",
        "        gradients =\n",
        "        # Almacenar o añadir el gradiente al gradiente del paso anterior de los parámetros\n",
        "        dh_prevt =\n",
        "        dx[:,:,t] =\n",
        "        dWr +=\n",
        "        dWu +=\n",
        "        dWh +=\n",
        "        dbu +=\n",
        "        dbr +=\n",
        "        dbh +=\n",
        "    # Establecer el gradiente de la primera activación al gradiente retropropagado dh_prev.\n",
        "    dh0 =\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    dWr /= m_batch; dWu /= m_batch; dWh /= m_batch\n",
        "    dbr /= m_batch; dbu /= m_batch; dbh /= m_batch\n",
        "    # Almacenar los gradientes en un diccionario de python\n",
        "    gradients = {\"dx\": dx, \"dh0\": dh0, \"dWr\": dWr,\"dbr\": dbr, \"dWu\": dWu,\"dbu\": dbu,\n",
        "                 \"dWh\": dWh, \"dbh\": dbh}\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "ozDNfFgNZttK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_backward_test(gru_backward,gru_cell_backward,gru_cell_forward)"
      ],
      "metadata": {
        "id": "U3O9pUKkYI6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inicialización de Parámetros de GRU\n",
        "La inicialización de los parámetros para el modelo GRU es exactamente la misma que utilizamos para inicializar los parámetros en LSTM. Solo que esta vez debes tener en cuenta que los pesos y los sesgos son distintos.\n",
        "\n",
        "#### Pistas:\n",
        "-  Recuerda inicializar los pesos utilizando [np.random.randn](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html), que tiene distribución Gaussiana $\\mathcal{N}(0,1)$, y a dicho tensor aplicarle el producto con la desviación estándar.\n",
        "\n",
        "$$ \\sigma = \\sqrt{\\frac{2}{N}}\\tag{1} $$"
      ],
      "metadata": {
        "id": "T3FaJvg70BaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: initialize_GRU_parateres\n",
        "\n",
        "def initialize_GRU_parameters(n_h, n_x, n_y):\n",
        "    \"\"\"\n",
        "    Inicializa los parámetros de la GRU con Xavier initialization\n",
        "\n",
        "    Arguments:\n",
        "    n_h -- número de unidades en el hidden state\n",
        "    n_x -- número de features de entrada\n",
        "    n_y -- número de features de salida\n",
        "\n",
        "    Returns:\n",
        "    parameters -- diccionario con los parámetros inicializados\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "\n",
        "    parameters = {}\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    parameters['Wu'] = np.random.randn(n_h, n_h + n_x) * np.sqrt(2.0 / (n_h + n_x))\n",
        "    parameters['bu'] = np.zeros((n_h, 1))\n",
        "\n",
        "    parameters['Wr'] = np.random.randn(n_h, n_h + n_x) * np.sqrt(2.0 / (n_h + n_x))\n",
        "    parameters['br'] = np.zeros((n_h, 1))\n",
        "\n",
        "    parameters['Wh'] = np.random.randn(n_h, n_h + n_x) * np.sqrt(2.0 / (n_h + n_x))\n",
        "    parameters['bh'] = np.zeros((n_h, 1))\n",
        "\n",
        "    parameters['Wy'] = np.random.randn(n_y, n_h) * np.sqrt(2.0 / n_h)\n",
        "    parameters['by'] = np.zeros((n_y, 1))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "mmdjAAxVYEme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initialize_GRU_parameters_test(initialize_GRU_parameters)"
      ],
      "metadata": {
        "id": "qNBQwaMkYenT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"4-5\"></a>\n",
        "### Ejercicio 14 - Entrenamiento del modelo GRU\n",
        "Ya tienes el modelo para hacer propagación hacia adelante y retropropagación hacia atrás. Ahora, con la inicialización de parámetros, es momento de implementar el entrenamiento del modelo GRU.\n",
        "\n",
        "En esta implementación vamos a utilizar el algoritmo de adam como optimizador con mini lotes -no te preocupes de esto, pues ya viene implementado y tu solo deberás programar los cálculos de la propagación y la retropropagación- y MSE como función de costo.\n",
        "\n",
        "Para este ejercicio es importante saber que nuestra función de costo se define\n",
        "\n",
        "$$\\mathcal{L} = \\frac{1}{2m_{batch}} \\sum_{i\\in batch}\\left(y_{pred} - y_{real}\\right)^2 \\tag{1}$$\n",
        "\n",
        "así, la derivada de esta función con respecto a la predicción $y_{pred}$ es de la forma\n",
        "\n",
        "$$d\\hat y =\\frac{d\\mathcal L}{d\\hat y} = \\frac{1}{m_{batch}} \\sum_{i\\in batch}\\left( y_{pred} - y_{real} \\right) \\tag{2}$$\n",
        "\n",
        "Con esto podemos calcular los parámetros de la salida y los valores del último paso del estado oculto $h^{\\langle T_x\\rangle}$. Su derivada es:\n",
        "\n",
        "$ d h^{\\langle T_x\\rangle} = W_y^T d \\hat y\\tag{3}$\n",
        "\n",
        "Y la derivada de los parámetros de $\\hat y$ se calculan de forma similar a los parámetros de las compuertas:\n",
        "\n",
        "$$ d W_y = d\\hat y (h^{\\langle t\\rangle})^T \\tag{4}$$\n",
        "$$ d b_y = \\sum_{batches} d \\hat y \\tag{5}$$\n",
        "\n",
        "Como ahora manejamos lotes, el número de muestras está dado por `m_batch`.\n",
        "\n",
        "**Instrucciones**:\n",
        "-   Obtén los valores del estado oculto `h`, la predicción `y_pred` y el cache `cache`, calculando la propagación hacia adelante.\n",
        "    - El estado oculto `h` tiene la forma `(n_h,m_batch,T_x)`.\n",
        "    - La predicción `y_pred` tiene la forma `(n_y,m_batch)`.\n",
        "-   Calcula el error de predicción llamando la función `compute_cost()` con las variables `y_pred` y `minibatch_Y`.\n",
        "    - La variable del mini lote `minibatch_Y` tiene forma `(n_y,m_batch,T_x)`.\n",
        "-   Inicializa la derivada $d h$ en la variable `dh`.\n",
        "    - Esta variable tiene la forma `(n_h,m_batch,T_x)`.\n",
        "-   Calcula la derivada de la función de costo utilizando la ecuación (2) en la variable `dcost_dy_pred`.\n",
        "-   Calcula la derivada del estado oculto $h^{\\langle T_x\\rangle}$ utilizando la ecuación (3).\n",
        "    -   Accede al paso $T_x$ con `h[:,:,-1]`.\n",
        "    -   Los valores de la propagación se han extraído anteriormente.\n",
        "-   Obtén los gradientes calculando la retropropagación.\n",
        "-   Actualiza el valor de las derivadas de los parámetros $W_y$ y $b_y$ utilizando las ecuaciones (4) y (5).\n",
        "\n",
        "#### Pistas:\n",
        "-  La función [np.zeros_like](https://numpy.org/devdocs/reference/generated/numpy.zeros_like.html) puede ser útil para inicializar los tensores.\n",
        "-  Utiliza la función [np.squeeze](https://numpy.org/devdocs/reference/generated/numpy.squeeze.html) para igualar dimensiones entre las variables `y_pred` y `minibatch_Y`."
      ],
      "metadata": {
        "id": "m6uKceZ80FNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función calificadora: model_gru_adam\n",
        "\n",
        "def model_gru_adam(X, Y, n_h, learning_rate=0.001, mini_batch_size=32,\n",
        "                    beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
        "                    num_epochs=1000, print_cost=True):\n",
        "  \"\"\"\n",
        "  Entrena una GRU desde cero con Adam y minibatches.\n",
        "\n",
        "  Arguments:\n",
        "  X -- datos de entrada, numpy array (n_x, m, T_x)\n",
        "  Y -- datos objetivo, numpy array (n_y, m, 1)\n",
        "  n_a -- número de unidades del estado oculto\n",
        "  learning_rate -- tasa de aprendizaje Adam\n",
        "  mini_batch_size -- tamaño de minibatch\n",
        "  num_epochs -- número de épocas\n",
        "  \"\"\"\n",
        "\n",
        "  n_x, m, T_x = X.shape\n",
        "  n_y, _, _ = Y.shape # Y la forma es (n_y, m, 1)\n",
        "\n",
        "  # Inicialización\n",
        "  parameters = initialize_GRU_parameters(n_h, n_x, n_y)\n",
        "  v, s = initialize_adam(parameters)\n",
        "  seed = 10\n",
        "  costs = []\n",
        "  t = 0  # contador de pasos de Adam\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    seed += 1\n",
        "    minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
        "    epoch_cost = 0\n",
        "\n",
        "    for minibatch_X, minibatch_Y in minibatches:\n",
        "      # Inicialización del estado oculto\n",
        "      m_batch = minibatch_X.shape[1]\n",
        "      h0 = np.zeros((n_h, m_batch))\n",
        "\n",
        "      ### START CODE HERE ###\n",
        "      # Forward pass utilizando tu función implementada anteriormente gru_forward(minibatch_X, h0, parameters)\n",
        "      h, y_pred, caches =\n",
        "\n",
        "      # Costo (MSE) - Calcula y_pred (n_y, m) con minibatch_Y (n_y, m, 1), utiliza .squeeze(axis=2) para igualar dimensiones.\n",
        "      cost =\n",
        "\n",
        "      # Inicializa las derivadas del estado oculto\n",
        "      dh =  # forma (n_h, m_batch, T_x)\n",
        "\n",
        "      # Calcula la derivada de la predicción de acuerdo a la ecuación (2). Recuerda utilizar .squeeze(axis=2) para igualar dimensiones.\n",
        "      dcost_dy_pred =\n",
        "\n",
        "      # Propaga la derivada al último bloque, utiliza la ecuación (3).\n",
        "      dh[:, :, -1] =\n",
        "\n",
        "      # Backward pass utilizando tu función implementada anteriormente lstm_backward(dh, caches, m_batch).\n",
        "      grads =\n",
        "\n",
        "      # Calcula las derivadas de los parámetros de la salida, utiliza las ecuaciones (4)-(5).\n",
        "      grads['dWy'] =\n",
        "      grads['dby'] =\n",
        "\n",
        "      ### END CODE HERE ###\n",
        "\n",
        "      # Recortar gradientes\n",
        "      grads = clip_gradients(grads, 5.0)\n",
        "\n",
        "      # Update Adam\n",
        "      t += 1\n",
        "      parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t,\n",
        "                                                        learning_rate, beta1, beta2, epsilon)\n",
        "\n",
        "      epoch_cost += cost / len(minibatches)\n",
        "\n",
        "    # Guardar costo\n",
        "    if print_cost:\n",
        "      print(f\"Cost after epoch {epoch}: {epoch_cost:.6f}\")\n",
        "      costs.append(epoch_cost)\n",
        "\n",
        "  # Graficar costos\n",
        "  if print_cost:\n",
        "    plot_training(costs)\n",
        "\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "Em_bwF1QaKHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru_adam_test(model_gru_adam)"
      ],
      "metadata": {
        "id": "jT85rk4wYllq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto ya puedes entrenar tu modelo GRU. Eres libre de experimentar con distintos hiperparámetros."
      ],
      "metadata": {
        "id": "hRLgZx_Odlby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modeloGRU = model_gru_adam(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    n_h= 78,\n",
        "    learning_rate=0.001,\n",
        "    mini_batch_size=64,\n",
        "    num_epochs=200,\n",
        "    print_cost=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "QfjRMnOnbM0h",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"4-6\"></a>\n",
        "### Predicción del modelo GRU\n",
        "Listo, ya haz entrenado tu modelo GRU. Sin embargo, ahora queremos hacer predicciones con este. Tu función `gru_forward` realiza una sola predicción sobre el número $T_x$ de pasos. Sin embargo, si queremos realizar predicciones sobre más instantes de tiempo, debemos actualizar las ventanas de tiempo y los valores de las ventanas con los valores predichos.\n",
        "\n",
        "Esta función es prácticamente la misma que ya haz implementado para LSTM, por lo que no deberás implementarla."
      ],
      "metadata": {
        "id": "fMYdskj80S8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_gru(window, parameters, steps=1):\n",
        "  \"\"\"\n",
        "  Realiza un número de predicciones con el modelo GRU empezando desde una ventana dada.\n",
        "\n",
        "  Argumentos:\n",
        "    window -- Ventana de entrada inicial para la predicción, shape (n_x, 1, T_x)\n",
        "    parameters -- Diccionario de Python con los parámetros entrenados (Wr, br, Wu, bu, ...)\n",
        "    steps -- Número de pasos de predicción a realizar\n",
        "\n",
        "    Retorna:\n",
        "    future_GRU_predictions -- Una lista con las predicciones.\n",
        "  \"\"\"\n",
        "  # Extraer dimensiones de la entrada.\n",
        "  n_x, m, T_x = window.shape\n",
        "  n_y, n_h = parameters[\"Wy\"].shape\n",
        "\n",
        "  # Lista para guardar las predicciones.\n",
        "  future_GRU_predictions_scaled = []\n",
        "  # Inicializa el primer estado oculto con ceros.\n",
        "  h0 = np.zeros((n_h, 1))\n",
        "\n",
        "  # Itera hasta el número de pasos dado en el parámetro steps.\n",
        "  for _ in range(steps):\n",
        "    # Realiza la prediccion sobre el conjunto de datos y de parámetros.\n",
        "    h, y, caches = gru_forward(window, h0, parameters)\n",
        "\n",
        "    # Guarda la prediccion, aplana el tensor 2D utilizando .flatten().\n",
        "    future_GRU_predictions_scaled.append(y.flatten()[0])\n",
        "\n",
        "    # Actualiza los valores de la ventana.\n",
        "    new_window_values = np.append(window.flatten()[1:], y.flatten())\n",
        "\n",
        "    # Redimensiona la ventana actual para mantener la forma (1,1,T_x).\n",
        "    window = new_window_values.reshape(1, 1, T_x)\n",
        "\n",
        "  #Desnormaliza los datos\n",
        "  future_GRU_predictions = scaler.inverse_transform(np.array(future_GRU_predictions_scaled).reshape(-1, 1))\n",
        "\n",
        "  return future_GRU_predictions.flatten()"
      ],
      "metadata": {
        "id": "je2o5xwEapjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tomamos la última ventana de datos conocida del conjunto de prueba\n",
        "current_window = X_test[:, -1:, :]\n",
        "test_GRU_prediction = predict_gru(current_window,modeloGRU,24)\n",
        "\n",
        "\n",
        "print(\"Predicciones de inflación para los próximos 24 meses:\")\n",
        "print(test_GRU_prediction.flatten())"
      ],
      "metadata": {
        "id": "9-ElgwyQdcuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Des-normalizar los datos para la gráfica\n",
        "y_train_actual = scaler.inverse_transform(y_train_keras.reshape(-1, 1))\n",
        "y_test_actual = scaler.inverse_transform(y_test_keras.reshape(-1, 1))\n",
        "\n",
        "# Crea un índice de fechas para la gráfica\n",
        "# Los índices de las secuencias corresponden a la *última* fecha de cada ventana.\n",
        "# Por lo tanto, y_train[i] corresponde a data.index[i + n_steps_in].\n",
        "start_train_index = n_steps_in\n",
        "end_train_index = start_train_index + train_size\n",
        "train_dates = data.index[start_train_index:end_train_index]\n",
        "\n",
        "start_test_index = end_train_index\n",
        "end_test_index = start_test_index + len(y_test_keras)\n",
        "test_dates = data.index[start_test_index:end_test_index]\n",
        "# Crear fechas futuras para las predicciones\n",
        "# La primera fecha futura es el mes siguiente a la última fecha de prueba\n",
        "future_dates = pd.date_range(start=test_dates[-1], periods=25, freq='ME')[1:]\n",
        "# Graficar los resultados\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Datos originales\n",
        "plt.plot(train_dates, y_train_actual, label='Datos de Entrenamiento (Reales)', color='blue')\n",
        "plt.plot(test_dates, y_test_actual, label='Datos de Prueba (Reales)', color='green')\n",
        "\n",
        "# Predicciones\n",
        "# future_predictions ya está desnormalizado\n",
        "plt.plot(future_dates, test_GRU_prediction, label='Predicción a 24 Meses', color='red', linestyle='--')\n",
        "\n",
        "# Estilo del gráfico\n",
        "plt.title('Predicción de Inflación con GRU', fontsize=16)\n",
        "plt.xlabel('Fecha', fontsize=12)\n",
        "plt.ylabel('Valor de Inflación', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wm-ZBwdcf2Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"5\"></a>\n",
        "## Implementación con Keras\n",
        "Ahora vamos a realizar una comparación entre nuestro modelo y uno creado con Keras.\n",
        "\n",
        "Primero cargamos las librerias necesarias."
      ],
      "metadata": {
        "id": "6kjOKmpOGxU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementación de LSTM"
      ],
      "metadata": {
        "id": "2S8Pn6mG0gi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modeloLSTM_Keras = Sequential()\n",
        "modeloLSTM_Keras.add(LSTM(units=78, input_shape=(X_train.shape[1], 1)))\n",
        "modeloLSTM_Keras.add(Dense(1))\n",
        "\n",
        "modeloLSTM_Keras.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "historyLSTM = modeloLSTM_Keras.fit(X_train_keras, y_train_keras, epochs=200, batch_size=64, verbose=1)"
      ],
      "metadata": {
        "id": "WocsI4YY4SV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar la función de pérdida\n",
        "plt.plot(historyLSTM.history['loss'])\n",
        "plt.title('Pérdida del modelo durante el entrenamiento')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.xlabel('Época')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MUPKfDM34vSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicción para el próximo año\n",
        "seq_lenght = 12 # n_steps_in\n",
        "\n",
        "# Tomar la última secuencia de los datos escalados para la predicción inicial\n",
        "current_window_keras = inflation_scaled[-n_steps_in:].reshape(1, n_steps_in, 1)\n",
        "\n",
        "# Lista para almacenar las predicciones de 24 meses\n",
        "future_LSTM_keras_predictions_scaled = []\n",
        "\n",
        "for _ in range(24):\n",
        "    # Hacer una predicción para el siguiente paso\n",
        "    next_step_pred_scaled = modeloLSTM_Keras.predict(current_window_keras, verbose=0)\n",
        "\n",
        "    # Almacenar la predicción\n",
        "    future_LSTM_keras_predictions_scaled.append(next_step_pred_scaled[0, 0])\n",
        "\n",
        "    # Actualizar la ventana: eliminar el valor más antiguo y añadir la nueva predicción\n",
        "    # Asegurar que current_window_keras permanezca (1, n_steps_in, 1)\n",
        "    new_window_values = np.append(current_window_keras.flatten()[1:], next_step_pred_scaled.flatten())\n",
        "    current_window_keras = new_window_values.reshape(1, n_steps_in, 1)\n",
        "\n",
        "# Desnormalizar todas las 24 predicciones\n",
        "future_LSTM_keras = scaler.inverse_transform(np.array(future_LSTM_keras_predictions_scaled).reshape(-1, 1))\n",
        "\n",
        "print(\"Predicciones de inflación para los próximos 24 meses con Keras:\")\n",
        "print(future_LSTM_keras.flatten())"
      ],
      "metadata": {
        "id": "y5ekZ5XR4vmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Des-normalizar los datos para la gráfica\n",
        "y_train_actual = scaler.inverse_transform(y_train_keras.reshape(-1, 1))\n",
        "y_test_actual = scaler.inverse_transform(y_test_keras.reshape(-1, 1))\n",
        "\n",
        "# Crea un índice de fechas para la gráfica\n",
        "# Los índices de las secuencias corresponden a la *última* fecha de cada ventana.\n",
        "# Por lo tanto, y_train[i] corresponde a data.index[i + n_steps_in].\n",
        "start_train_index = n_steps_in\n",
        "end_train_index = start_train_index + train_size\n",
        "train_dates = data.index[start_train_index:end_train_index]\n",
        "\n",
        "start_test_index = end_train_index\n",
        "end_test_index = start_test_index + len(y_test_keras)\n",
        "test_dates = data.index[start_test_index:end_test_index]\n",
        "# Crear fechas futuras para las predicciones\n",
        "# La primera fecha futura es el mes siguiente a la última fecha de prueba\n",
        "future_dates = pd.date_range(start=test_dates[-1], periods=25, freq='ME')[1:]\n",
        "# Graficar los resultados\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Datos originales\n",
        "plt.plot(train_dates, y_train_actual, label='Datos de Entrenamiento (Reales)', color='blue')\n",
        "plt.plot(test_dates, y_test_actual, label='Datos de Prueba (Reales)', color='green')\n",
        "\n",
        "# Predicciones\n",
        "# future_predictions ya está desnormalizado\n",
        "plt.plot(future_dates, future_LSTM_keras.flatten(), label='Predicción a 24 Meses (Keras)', color='red', linestyle='--')\n",
        "\n",
        "# Estilo del gráfico\n",
        "plt.title('Predicción de Inflación con LSTM y Keras', fontsize=16)\n",
        "plt.xlabel('Fecha', fontsize=12)\n",
        "plt.ylabel('Valor de Inflación', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-QtK2o71hzCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementación de GRU"
      ],
      "metadata": {
        "id": "3Sq6kK1abafk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modeloGRU_Keras = Sequential()\n",
        "modeloGRU_Keras.add(GRU(units=78, input_shape=(X_train.shape[1], 1)))\n",
        "modeloGRU_Keras.add(Dense(1))\n",
        "\n",
        "modeloGRU_Keras.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "historyGRU = modeloGRU_Keras.fit(X_train_keras, y_train_keras, epochs=200, batch_size=64, verbose=1)"
      ],
      "metadata": {
        "id": "XQJljP5zbb-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar la función de pérdida\n",
        "plt.plot(historyGRU.history['loss'])\n",
        "plt.title('Pérdida del modelo durante el entrenamiento')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.xlabel('Época')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "brEPt8IgiTEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicción para el próximo año\n",
        "seq_lenght = 12 # n_steps_in\n",
        "\n",
        "# Tomar la última secuencia de los datos escalados para la predicción inicial\n",
        "current_window_keras = inflation_scaled[-n_steps_in:].reshape(1, n_steps_in, 1)\n",
        "\n",
        "# Lista para almacenar las predicciones de 24 meses\n",
        "future_GRU_keras_predictions_scaled = []\n",
        "\n",
        "for _ in range(24):\n",
        "    # Hacer una predicción para el siguiente paso\n",
        "    next_step_pred_scaled = modeloGRU_Keras.predict(current_window_keras, verbose=0)\n",
        "\n",
        "    # Almacenar la predicción\n",
        "    future_GRU_keras_predictions_scaled.append(next_step_pred_scaled[0, 0])\n",
        "\n",
        "    # Actualizar la ventana: eliminar el valor más antiguo y añadir la nueva predicción\n",
        "    # Asegurar que current_window_keras permanezca (1, n_steps_in, 1)\n",
        "    new_window_values = np.append(current_window_keras.flatten()[1:], next_step_pred_scaled.flatten())\n",
        "    current_window_keras = new_window_values.reshape(1, n_steps_in, 1)\n",
        "\n",
        "# Desnormalizar todas las 24 predicciones\n",
        "future_GRU_keras = scaler.inverse_transform(np.array(future_GRU_keras_predictions_scaled).reshape(-1, 1))\n",
        "\n",
        "print(\"Predicciones de inflación para los próximos 24 meses con Keras:\")\n",
        "print(future_GRU_keras.flatten())"
      ],
      "metadata": {
        "id": "gI3tYRwmiZlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Des-normalizar los datos para la gráfica\n",
        "y_train_actual = scaler.inverse_transform(y_train_keras.reshape(-1, 1))\n",
        "y_test_actual = scaler.inverse_transform(y_test_keras.reshape(-1, 1))\n",
        "\n",
        "# Crea un índice de fechas para la gráfica\n",
        "# Los índices de las secuencias corresponden a la *última* fecha de cada ventana.\n",
        "# Por lo tanto, y_train[i] corresponde a data.index[i + n_steps_in].\n",
        "start_train_index = n_steps_in\n",
        "end_train_index = start_train_index + train_size\n",
        "train_dates = data.index[start_train_index:end_train_index]\n",
        "\n",
        "start_test_index = end_train_index\n",
        "end_test_index = start_test_index + len(y_test_keras)\n",
        "test_dates = data.index[start_test_index:end_test_index]\n",
        "# Crear fechas futuras para las predicciones\n",
        "# La primera fecha futura es el mes siguiente a la última fecha de prueba\n",
        "future_dates = pd.date_range(start=test_dates[-1], periods=25, freq='ME')[1:]\n",
        "# Graficar los resultados\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Datos originales\n",
        "plt.plot(train_dates, y_train_actual, label='Datos de Entrenamiento (Reales)', color='blue')\n",
        "plt.plot(test_dates, y_test_actual, label='Datos de Prueba (Reales)', color='green')\n",
        "\n",
        "# Predicciones\n",
        "# future_predictions ya está desnormalizado\n",
        "plt.plot(future_dates, future_GRU_keras.flatten(), label='Predicción a 24 Meses', color='red', linestyle='--')\n",
        "\n",
        "# Estilo del gráfico\n",
        "plt.title('Predicción de Inflación con GRU y Keras', fontsize=16)\n",
        "plt.xlabel('Fecha', fontsize=12)\n",
        "plt.ylabel('Valor de Inflación', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C37APAJ0ikOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eb74bb2"
      },
      "source": [
        "### Actualizar predicciones (Keras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "291bb758"
      },
      "source": [
        "m_test_lstm = X_test.shape[1]\n",
        "h0_test_lstm = np.zeros((modeloLSTM['Wf'].shape[0], m_test_lstm))\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba con el modelo LSTM implementado\n",
        "_, y_pred_test_lstm_scaled, _, _ = lstm_forward(X_test, h0_test_lstm, modeloLSTM)\n",
        "\n",
        "# Des-normalizar las predicciones del conjunto de prueba\n",
        "y_pred_test_lstm_actual = scaler.inverse_transform(y_pred_test_lstm_scaled.reshape(-1, 1))\n",
        "\n",
        "# Concatenar las predicciones del conjunto de prueba con las futuras\n",
        "combined_LSTM_predictions = np.concatenate((y_pred_test_lstm_actual.squeeze(), test_LSTM_prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ccf822"
      },
      "source": [
        "# Realizar predicciones en el conjunto de prueba con el modelo LSTM de Keras\n",
        "y_pred_test_lstm_keras_scaled = modeloLSTM_Keras.predict(X_test_keras, verbose=0)\n",
        "\n",
        "# Des-normalizar las predicciones del conjunto de prueba\n",
        "y_pred_test_lstm_keras_actual = scaler.inverse_transform(y_pred_test_lstm_keras_scaled.reshape(-1, 1))\n",
        "\n",
        "# Concatenar las predicciones del conjunto de prueba con las futuras\n",
        "combined_LSTM_keras_predictions = np.concatenate((y_pred_test_lstm_keras_actual, future_LSTM_keras))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6945e35"
      },
      "source": [
        "m_test_gru = X_test.shape[1]\n",
        "h0_test_gru = np.zeros((modeloGRU['Wu'].shape[0], m_test_gru))\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba con el modelo GRU implementado\n",
        "_, y_pred_test_gru_scaled, _ = gru_forward(X_test, h0_test_gru, modeloGRU)\n",
        "\n",
        "# Des-normalizar las predicciones del conjunto de prueba\n",
        "y_pred_test_gru_actual = scaler.inverse_transform(y_pred_test_gru_scaled.reshape(-1, 1))\n",
        "\n",
        "# Concatenar las predicciones del conjunto de prueba con las futuras\n",
        "combined_GRU_predictions = np.concatenate((y_pred_test_gru_actual.squeeze(), test_GRU_prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d03df1d"
      },
      "source": [
        "# Realizar predicciones en el conjunto de prueba con el modelo GRU de Keras\n",
        "y_pred_test_gru_keras_scaled = modeloGRU_Keras.predict(X_test_keras, verbose=0)\n",
        "\n",
        "# Des-normalizar las predicciones del conjunto de prueba\n",
        "y_pred_test_gru_keras_actual = scaler.inverse_transform(y_pred_test_gru_keras_scaled.reshape(-1, 1))\n",
        "\n",
        "# Concatenar las predicciones del conjunto de prueba con las futuras\n",
        "combined_GRU_keras_predictions = np.concatenate((y_pred_test_gru_keras_actual, future_GRU_keras))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67c3ce5d"
      },
      "source": [
        "y_train_actual = scaler.inverse_transform(y_train_keras.reshape(-1, 1))\n",
        "start_train_index = n_steps_in\n",
        "end_train_index = start_train_index + train_size\n",
        "train_dates = data.index[start_train_index:end_train_index]\n",
        "start_test_index = end_train_index\n",
        "end_test_index = start_test_index + len(y_test_keras)\n",
        "test_dates = data.index[start_test_index:end_test_index]\n",
        "combined_prediction_dates = pd.date_range(start=test_dates[0], periods=len(y_test_keras) + 24, freq='ME')\n",
        "plt.figure(figsize=(13, 7))\n",
        "plt.plot(test_dates, y_test_actual, label='Datos de Prueba (Reales)', color='green',linewidth=2, alpha=0.7)\n",
        "plt.plot(combined_prediction_dates, combined_LSTM_predictions.flatten(), label='Predicción LSTM (Implementada)', color='red', linestyle='--')\n",
        "plt.plot(combined_prediction_dates, combined_LSTM_keras_predictions.flatten(), label='Predicción LSTM (Keras)', color='orange', linestyle='--')\n",
        "plt.plot(combined_prediction_dates, combined_GRU_predictions.flatten(), label='Predicción GRU (Implementada)', color='blue', linestyle='--')\n",
        "plt.plot(combined_prediction_dates, combined_GRU_keras_predictions.flatten(), label='Predicción GRU (Keras)', color='brown', linestyle='--')\n",
        "plt.axvline(x=test_dates[-1], color='black', linestyle=':', linewidth=1, label='Fin datos de prueba')\n",
        "plt.axvline(x=combined_prediction_dates[len(y_test_keras) + 11], color='black', linestyle='--', linewidth=1, label='Fin 12 meses futuros')\n",
        "plt.xlim(test_dates[0], combined_prediction_dates[-1])\n",
        "plt.title('Predicciones de Inflación a 24 Meses (Todos los Modelos)', fontsize=16)\n",
        "plt.xlabel('Fecha', fontsize=12)\n",
        "plt.ylabel('Valor de Inflación', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "#plt.savefig(\"predicciones3.png\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cde17d5"
      },
      "source": [
        "<a name=\"6\"></a>\n",
        "## Predicción en intervalos de confianza (Opcional)\n",
        "\n",
        "Una vez entrenados los modelos LSTM y GRU, es útil no solo obtener predicciones puntuales sino también cuantificar la incertidumbre asociada a dichas predicciones. Los intervalos de confianza nos permiten establecer un rango dentro del cual esperamos que se encuentre el valor real con cierta probabilidad (típicamente 95%).\n",
        "\n",
        "Para construir estos intervalos utilizaremos un método de **bootstrap residual**, que consiste en:\n",
        "\n",
        "1. Calcular los residuos del conjunto de prueba (diferencia entre valores reales y predichos).\n",
        "2. Generar múltiples trayectorias de predicción futura, donde cada trayectoria se ajusta añadiendo residuos seleccionados aleatoriamente.\n",
        "3. Calcular la media y los percentiles de todas las trayectorias generadas para obtener la predicción promedio y los límites del intervalo.\n",
        "\n",
        "Este enfoque captura la variabilidad observada en los errores del modelo durante el entrenamiento y la proyecta hacia las predicciones futuras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e716a5cb"
      },
      "source": [
        "### Calcular Residuos del Conjunto de Prueba\n",
        "\n",
        "El primer paso es calcular los residuos, que representan los errores que el modelo cometió en el conjunto de prueba. Estos residuos serán la base para estimar la incertidumbre en predicciones futuras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad1f20c"
      },
      "source": [
        "residuals_lstm = y_test_actual - y_pred_test_lstm_actual\n",
        "residuals_gru = y_test_actual - y_pred_test_gru_actual\n",
        "\n",
        "print(\"Residuals for implemented LSTM model:\\n\", residuals_lstm[:5])\n",
        "print(\"\\nResiduals for implemented GRU model:\\n\", residuals_gru[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los residuos muestran qué tan lejos estuvieron las predicciones del modelo de los valores reales. Un buen modelo tendrá residuos pequeños y distribuidos aleatoriamente alrededor de cero."
      ],
      "metadata": {
        "id": "y1NBLPGTa29W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio Opcional - Función de Bootstrap para Intervalos de Confianza\n",
        "\n",
        "Implementa la función `bootstrap_prediction_intervals` que genera predicciones futuras con intervalos de confianza utilizando el método de bootstrap residual.\n",
        "\n",
        "El método funciona de la siguiente manera:\n",
        "1. Para cada muestra de bootstrap, comenzamos con la última ventana de datos observados.\n",
        "2. Realizamos predicciones paso a paso hacia el futuro.\n",
        "3. En cada paso, ajustamos la predicción base del modelo añadiendo un residuo seleccionado aleatoriamente del conjunto de residuos de prueba.\n",
        "4. Actualizamos la ventana deslizante con la predicción ajustada para continuar con el siguiente paso.\n",
        "5. Repetimos este proceso múltiples veces (típicamente 100 o más) para generar un conjunto de trayectorias posibles.\n",
        "6. Finalmente, calculamos la media y los percentiles 2.5% y 97.5% de todas las trayectorias para obtener la predicción promedio y un intervalo de confianza del 95%.\n",
        "\n",
        "**Instrucciones**:\n",
        "-  Implementa el bucle interno que realiza predicciones para cada paso futuro (`n_steps_out`).\n",
        "-  Para cada paso de predicción:\n",
        "    1. Realiza la predicción base utilizando la función `predict_lstm` o `predict_gru` según el tipo de modelo.\n",
        "        - Usa `current_window_bootstrap` como entrada.\n",
        "        - La predicción estará escalada (normalizada).\n",
        "    2. Selecciona un residuo aleatorio del vector `residuals_flat` utilizando `np.random.choice()`.\n",
        "        - Este residuo también está escalado, por lo que es compatible con la predicción escalada.\n",
        "    3. Ajusta la predicción base sumándole el residuo aleatorio.\n",
        "        - Usa `.flatten()[0]` para extraer el valor escalar de la predicción.\n",
        "        - Guarda el resultado en `adjusted_pred_scaled`.\n",
        "    4. Agrega la predicción ajustada a la lista `current_bootstrap_predictions_scaled`.\n",
        "    5. Actualiza la ventana deslizante para la siguiente iteración:\n",
        "        - Elimina el primer valor de la ventana actual usando `current_window_bootstrap.flatten()[1:]`.\n",
        "        - Añade la predicción ajustada al final usando `np.append()`.\n",
        "        - Reorganiza el resultado en forma `(1, 1, n_steps_in)` usando `.reshape()`.\n",
        "\n",
        "#### Pistas:\n",
        "-  Usa `np.random.choice(residuals_flat)` para seleccionar un residuo aleatorio con reemplazo.\n",
        "-  Usa `np.append(array1, value)` para añadir un valor al final de un array.\n",
        "-  Recuerda que las predicciones de los modelos tienen forma `(n_y, m)`, por lo que necesitas usar `.flatten()[0]` para extraer el valor escalar.\n",
        "-  La ventana debe mantener siempre el mismo tamaño `n_steps_in`, por lo que debes eliminar el primer elemento antes de añadir el nuevo al final.\n",
        "\n"
      ],
      "metadata": {
        "id": "eB3Uk8Cna7kb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "655fac5d"
      },
      "source": [
        "import copy\n",
        "\n",
        "def bootstrap_prediction_intervals(model, parameters, scaler, last_window, n_steps_in, n_steps_out, residuals, n_bootstrap_samples=100, model_type='lstm'):\n",
        "    \"\"\"\n",
        "    Genera predicciones futuras con intervalos de confianza utilizando un método de bootstrap.\n",
        "\n",
        "    Argumentos:\n",
        "    model -- La función de predicción del modelo (e.g., predict_lstm o predict_gru).\n",
        "    parameters -- Diccionario con los parámetros entrenados del modelo.\n",
        "    scaler -- El objeto StandardScaler utilizado para normalizar los datos.\n",
        "    last_window -- La última ventana de datos escalados para iniciar la predicción, shape (1, 1, n_steps_in).\n",
        "    n_steps_in -- Número de pasos de entrada de la ventana (para el reshape).\n",
        "    n_steps_out -- Número de pasos a predecir (horizonte de predicción).\n",
        "    residuals -- Residuos del conjunto de prueba (diferencia entre real y predicho), shape (num_residuals, 1).\n",
        "    n_bootstrap_samples -- Número de muestras de bootstrap a generar.\n",
        "    model_type -- Tipo de modelo ('lstm' o 'gru') para seleccionar la función de predicción.\n",
        "\n",
        "    Retorna:\n",
        "    mean_predictions -- La predicción media desnormalizada para cada paso futuro, shape (n_steps_out, 1).\n",
        "    lower_bound -- El límite inferior del intervalo de confianza desnormalizado, shape (n_steps_out, 1).\n",
        "    upper_bound -- El límite superior del intervalo de confianza desnormalizado, shape (n_steps_out, 1).\n",
        "    \"\"\"\n",
        "\n",
        "    all_bootstrap_predictions_scaled = []\n",
        "    residuals_flat = residuals.flatten()\n",
        "\n",
        "    for _ in range(n_bootstrap_samples):\n",
        "        current_window_bootstrap = copy.deepcopy(last_window)\n",
        "        current_bootstrap_predictions_scaled = []\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        for _ in range( ??? ):\n",
        "            # Hacer la predicción base (escalada). Utiliza tus funciones predict_lstm y predict_gru según sea el caso.\n",
        "            if model_type == 'lstm':\n",
        "                next_step_pred_scaled =\n",
        "            else:     # GRU\n",
        "                next_step_pred_scaled =\n",
        "\n",
        "            # Seleccionar un residuo aleatorio con reemplazo\n",
        "            random_residual =\n",
        "\n",
        "            # Ajustar la predicción con el residuo\n",
        "            adjusted_pred_scaled =\n",
        "\n",
        "            current_bootstrap_predictions_scaled.append( ??? )\n",
        "\n",
        "            # Actualizar la ventana para la siguiente predicción\n",
        "            # Asegurarse de que el new_window_values tenga la forma correcta antes de reshape\n",
        "            new_window_values = np.append( ??? )\n",
        "            current_window_bootstrap =\n",
        "\n",
        "            ### END CODE HERE ###\n",
        "\n",
        "        all_bootstrap_predictions_scaled.append(current_bootstrap_predictions_scaled)\n",
        "\n",
        "    # Convertir la lista de predicciones bootstrap a un array numpy\n",
        "    all_bootstrap_predictions_scaled = np.array(all_bootstrap_predictions_scaled)\n",
        "\n",
        "    # Calcular la media y los percentiles para cada paso de tiempo\n",
        "    mean_predictions_scaled = np.mean(all_bootstrap_predictions_scaled, axis=0).reshape(-1, 1)\n",
        "    lower_bound_scaled = np.percentile(all_bootstrap_predictions_scaled, 2.5, axis=0).reshape(-1, 1)\n",
        "    upper_bound_scaled = np.percentile(all_bootstrap_predictions_scaled, 97.5, axis=0).reshape(-1, 1)\n",
        "\n",
        "    # Desnormalizar las predicciones\n",
        "    mean_predictions = scaler.inverse_transform(mean_predictions_scaled)\n",
        "    lower_bound = scaler.inverse_transform(lower_bound_scaled)\n",
        "    upper_bound = scaler.inverse_transform(upper_bound_scaled)\n",
        "\n",
        "    return mean_predictions, lower_bound, upper_bound\n",
        "\n",
        "print(\"bootstrap_prediction_intervals function defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47411d8"
      },
      "source": [
        "### Generar Intervalos de Predicción LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9438e0ec"
      },
      "source": [
        "last_window_lstm = X_test[:, -1:, :]\n",
        "\n",
        "lstm_mean_predictions, lstm_lower_bound, lstm_upper_bound = bootstrap_prediction_intervals(\n",
        "    model=predict_lstm,\n",
        "    parameters=modeloLSTM,\n",
        "    scaler=scaler,\n",
        "    last_window=last_window_lstm,\n",
        "    n_steps_in=n_steps_in,\n",
        "    n_steps_out=24,\n",
        "    residuals=residuals_lstm,\n",
        "    n_bootstrap_samples=500,\n",
        "    model_type='lstm'\n",
        ")\n",
        "\n",
        "print(\"LSTM Mean Predictions (next 24 months):\")\n",
        "print(lstm_mean_predictions.flatten()[:5], \"...\")\n",
        "print(\"\\nLSTM Lower Bound (next 24 months):\")\n",
        "print(lstm_lower_bound.flatten()[:5], \"...\")\n",
        "print(\"\\nLSTM Upper Bound (next 24 months):\")\n",
        "print(lstm_upper_bound.flatten()[:5], \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5bb1ebb"
      },
      "source": [
        "### Generar Intervalos de Predicción GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8de74ba"
      },
      "source": [
        "last_window_gru = X_test[:, -1:, :]\n",
        "\n",
        "gru_mean_predictions, gru_lower_bound, gru_upper_bound = bootstrap_prediction_intervals(\n",
        "    model=predict_gru,\n",
        "    parameters=modeloGRU,\n",
        "    scaler=scaler,\n",
        "    last_window=last_window_gru,\n",
        "    n_steps_in=n_steps_in,\n",
        "    n_steps_out=24,\n",
        "    residuals=residuals_gru,\n",
        "    n_bootstrap_samples=500,\n",
        "    model_type='gru'\n",
        ")\n",
        "\n",
        "print(\"GRU Mean Predictions (next 24 months):\")\n",
        "print(gru_mean_predictions.flatten()[:5], \"...\")\n",
        "print(\"\\nGRU Lower Bound (next 24 months):\")\n",
        "print(gru_lower_bound.flatten()[:5], \"...\")\n",
        "print(\"\\nGRU Upper Bound (next 24 months):\")\n",
        "print(gru_upper_bound.flatten()[:5], \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6788e141"
      },
      "source": [
        "### Graficar Predicciones LSTM con Intervalos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "466a0720"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Des-normalizar los datos para la gráfica\n",
        "y_train_actual = scaler.inverse_transform(y_train_keras.reshape(-1, 1))\n",
        "y_test_actual = scaler.inverse_transform(y_test_keras.reshape(-1, 1))\n",
        "start_train_index = n_steps_in\n",
        "end_train_index = start_train_index + train_size\n",
        "train_dates = data.index[start_train_index:end_train_index]\n",
        "\n",
        "start_test_index = end_train_index\n",
        "end_test_index = start_test_index + len(y_test_keras)\n",
        "test_dates = data.index[start_test_index:end_test_index]\n",
        "combined_prediction_dates = pd.date_range(start=test_dates[0], periods=len(y_test_keras) + 24, freq='ME')\n",
        "combined_lstm_bootstrap_mean_predictions = np.concatenate((y_pred_test_lstm_actual, lstm_mean_predictions))\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.plot(test_dates, y_test_actual, label='Datos de Prueba (Reales)', color='green', linewidth=2)\n",
        "plt.plot(combined_prediction_dates, combined_lstm_bootstrap_mean_predictions.flatten(), label='Predicción LSTM (Implementada) con IC', color='red', linestyle='-')\n",
        "future_dates_only = pd.date_range(start=test_dates[-1], periods=25, freq='ME')[1:]\n",
        "plt.fill_between(future_dates_only, lstm_lower_bound.flatten(), lstm_upper_bound.flatten(), color='red', alpha=0.2, label='Intervalo de Confianza LSTM (95%)')\n",
        "plt.plot(combined_prediction_dates, combined_LSTM_keras_predictions.flatten(), label='Predicción LSTM (Keras)', color='orange', linestyle='--')\n",
        "plt.axvline(x=test_dates[-1], color='black', linestyle=':', linewidth=1, label='Fin datos de prueba')\n",
        "plt.axvline(x=combined_prediction_dates[len(y_test_keras) + 11], color='black', linestyle='--', linewidth=1, label='Fin 12 meses futuros')\n",
        "plt.xlim(test_dates[0], combined_prediction_dates[-1])\n",
        "plt.title('Predicción de Inflación con LSTM con IC', fontsize=16)\n",
        "plt.xlabel('Fecha', fontsize=12)\n",
        "plt.ylabel('Valor de Inflación', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"lstmIC.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e59408ec"
      },
      "source": [
        "### Graficar Predicciones GRU con Intervalos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d018435d"
      },
      "source": [
        "# Des-normalizar los datos para la gráfica\n",
        "y_train_actual = scaler.inverse_transform(y_train_keras.reshape(-1, 1))\n",
        "y_test_actual = scaler.inverse_transform(y_test_keras.reshape(-1, 1))\n",
        "\n",
        "# Crea un índice de fechas para la gráfica\n",
        "start_train_index = n_steps_in\n",
        "end_train_index = start_train_index + train_size\n",
        "train_dates = data.index[start_train_index:end_train_index]\n",
        "\n",
        "start_test_index = end_train_index\n",
        "end_test_index = start_test_index + len(y_test_keras)\n",
        "test_dates = data.index[start_test_index:end_test_index]\n",
        "\n",
        "# Crear fechas futuras para las predicciones\n",
        "future_dates_only = pd.date_range(start=test_dates[-1], periods=25, freq='ME')[1:]\n",
        "combined_prediction_dates = pd.date_range(start=test_dates[0], periods=len(y_test_keras) + 24, freq='ME')\n",
        "combined_gru_bootstrap_mean_predictions = np.concatenate((y_pred_test_gru_actual, gru_mean_predictions))\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.plot(test_dates, y_test_actual, label='Datos de Prueba (Reales)', color='green', linewidth=2)\n",
        "plt.plot(combined_prediction_dates, combined_gru_bootstrap_mean_predictions.flatten(), label='Predicción GRU (Implementada) con IC', color='blue', linestyle='-')\n",
        "plt.fill_between(future_dates_only, gru_lower_bound.flatten(), gru_upper_bound.flatten(), color='blue', alpha=0.2, label='Intervalo de Confianza GRU (95%)')\n",
        "plt.plot(combined_prediction_dates, combined_GRU_keras_predictions.flatten(), label='Predicción GRU (Keras)', color='c', linestyle='--')\n",
        "plt.axvline(x=test_dates[-1], color='black', linestyle=':', linewidth=1, label='Fin datos de prueba')\n",
        "plt.axvline(x=combined_prediction_dates[len(y_test_keras) + 11], color='black', linestyle='--', linewidth=1, label='Fin 12 meses futuros')\n",
        "plt.xlim(test_dates[0], combined_prediction_dates[-1])\n",
        "plt.title('Predicción de Inflación con GRU con IC', fontsize=16)\n",
        "plt.xlabel('Fecha', fontsize=12)\n",
        "plt.ylabel('Valor de Inflación', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "#plt.savefig(\"gruIC.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"7\"></a>\n",
        "## Bibliografia\n",
        "\n",
        "- Ortiz Higuera, M. S. (2025). Construcción de un laboratorio interactivo para el modelamiento de Redes Neuronales Recurrentes con aplicaciones en actuaría y finanzas. *Tesis de pregrado, Universidad Nacional de Colombia*. Facultad de Ciencias, Departamento de Matemáticas, Sede Bogotá.\n",
        "\n",
        "- Cárdenas-Cárdenas, J. A., Cristiano-Botia, D. J., Martínez-Cortés, N., & others. (2023). Colombian inflation forecast using long short-term memory approach. *Borradores de Economía; No. 1241*. Banco de la República de Colombia.\n",
        "\n",
        "- Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. *Neural Computation, 9*(8), 1735-1780. https://doi.org/10.1162/neco.1997.9.8.1735\n",
        "\n",
        "- Ng, A., & deeplearning.ai. (2018). Sequence Models. Coursera. https://www.coursera.org/learn/nlp-sequence-models (Accedido el 10 de Septiembre de 2025)\n",
        "\n",
        "- Cho, K., van Merrienboer, B., Bahdanau, D., & Bengio, Y. (2014). On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. *arXiv preprint arXiv:1409.1259*. https://arxiv.org/abs/1409.1259"
      ],
      "metadata": {
        "id": "tMvSrIMv7VMz"
      }
    }
  ]
}